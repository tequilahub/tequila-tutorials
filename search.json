[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Tequila Tutorials",
    "section": "",
    "text": "Welcome to the Official Website of Tequila Tutorials\nTequila is an abstraction framework for (variational) quantum algorithms. It operates on abstract data structures allowing the formulation, combination, automatic differentiation and optimization of generalized objectives. Tequila can execute the underlying quantum expectation values on state of the art simulators as well as on real quantum devices.\nThis website contains some hands-on examples using tequila. A good starting point from a very fundamental tutorial is here.\nThe main sections of this website are:\n\na wide collection of tutorials for getting to know tequila’s basic functionalities and general usage\na collection of more specific tutorials regarding research\na page with frequently asked questions regarding various aspects and usage of tequila FAQ\n\n\n\nContribute\nTequila is free and open source. You’re welcome to contribute if you have ideas to improve the library. The standard way to contribute is via pull-requests or issues on github. For larger projects it might be useful to let us know in advance what you are planning.\n\n\nInfluences\nThe design of tequilas API was inspired by madness. Agnostic backend handling and forcing differentiability was inspired by pennylane."
  },
  {
    "objectID": "FAQ/FAQ.html",
    "href": "FAQ/FAQ.html",
    "title": "FAQ",
    "section": "",
    "text": "Frequently Asked Questions\n\nIt is recommended to take a look at the BasicUsage notebook before looking at this\n\nimport tequila as tq\nimport numpy\n\n\n\n\nWhich optimization methods can I use?\ntq.show_available_optimizers shows you all installed modules on your systems and the methods which tq.minimize understands. Method names are not case sensitive when passed to tq.minimize.\nIn the end you see which modules are supported and which of them are actually installed on your system.\nThe table with methods and modules will only show you the methods for modules that are currently installed within your environment.\nOf course you can also use tequila objectives for your own optimizers.\nYou don’t need to use the modules here.\n\ntq.show_available_optimizers()\n\navailable methods for optimizer modules found on your system:\nmethod               | optimizer module\n--------------------------\nNELDER-MEAD          | scipy\nCOBYLA               | scipy\nPOWELL               | scipy\nSLSQP                | scipy\nL-BFGS-B             | scipy\nBFGS                 | scipy\nCG                   | scipy\nTNC                  | scipy\nTRUST-KRYLOV         | scipy\nNEWTON-CG            | scipy\nDOGLEG               | scipy\nTRUST-NCG            | scipy\nTRUST-EXACT          | scipy\nTRUST-CONSTR         | scipy\nadam                 | gd\nadagrad              | gd\nadamax               | gd\nnadam                | gd\nsgd                  | gd\nmomentum             | gd\nnesterov             | gd\nrmsprop              | gd\nrmsprop-nesterov     | gd\nspsa                 | gd\ngpyopt-lbfgs         | gpyopt\ngpyopt-direct        | gpyopt\ngpyopt-cma           | gpyopt\nSupported optimizer modules:  ['scipy', 'gpyopt', 'gd']\nInstalled optimizer modules:  ['scipy', 'gd', 'gpyopt']\n\n\n\n\nWhich simulators/Quantum-Backends can I use?\ntq.show_available_simulators shows all simulators/quantum backends which are supported by tequila as well as which are installed within your current environment.\nThe default choice if you don’t specify a backend when for example simulating a tequila objective with tq.simulate is the first entry of the supported backends which is installed on your system.\n\ntq.show_available_simulators()\n\nbackend         | wfn        | sampling   | noise      | installed \n--------------------------------------------------------------------\nqulacs_gpu      | False      | False      | False      | False     \nqulacs          | True       | True       | True       | True      \nqibo            | False      | False      | False      | False     \nqiskit          | False      | False      | False      | False     \ncirq            | False      | False      | False      | False     \npyquil          | True       | True       | True       | True      \nsymbolic        | True       | False      | False      | True      \nqlm             | False      | False      | False      | False     \n\n\n\n\nCan I avoid re-translation/compilation on my objectives/circuits?\nYes, you can. By calling tq.compile instead of tq.simulate. This will give you back a callable objective.\nCheck also the basic usage tutorial notebook\n\nU = tq.gates.H(target=1) + tq.gates.Rx(angle=\"a\", target=0, control=1)\n\n# simulate the wavefunction with different variables\nwfn0 = tq.simulate(U, variables={\"a\": 1.0})\nwfn1 = tq.simulate(U, variables={\"a\": 2.0})\n\nprint(wfn0)\nprint(wfn1)\n\n# the same, but avoiding re-compilation\n# Note that your compiled object is translated to a quantum backend\n# if the backend was not set, tequila it will pick the default which depends\n# on which backends you have installed. You will seee it in the printout of the\n# compiled circuits\ncompiled_U = tq.compile(U)\nwfn0 = compiled_U(variables={\"a\":1.0})\nwfn1 = compiled_U(variables={\"a\":2.0})\n\nprint(\"compiled circuit:\", compiled_U)\nprint(wfn0)\nprint(wfn1)\n\n\n# With Objectives it works in the same way\nH = tq.paulis.Y(0)\nE = tq.ExpectationValue(H=H, U=U)\nobjective = E**2 + 1.0\n\n# simulate the objective with different variables\nresult0 = tq.simulate(objective, variables={\"a\": 1.0})\nresult1 = tq.simulate(objective, variables={\"a\": 2.0})\n\nprint(\"compiled objective:\", objective)\nprint(result0)\nprint(result1)\n\n# compile and then simulate\ncompiled_objective = tq.compile(objective)\nresult0 = compiled_objective(variables={\"a\":1.0})\nresult1 = compiled_objective(variables={\"a\":2.0})\n\nprint(\"compiled objective:\", compiled_objective)\nprint(result0)\nprint(result1)\n\n+0.7071|00&gt; +0.6205|01&gt; -0.3390i|11&gt; \n+0.7071|00&gt; +0.3821|01&gt; -0.5950i|11&gt; \ncompiled circuit: &lt;tequila.simulators.simulator_qulacs.BackendCircuitQulacs object at 0x3075b72e0&gt;\n+0.7071|00&gt; +0.6205|01&gt; -0.3390i|11&gt; \n+0.7071|00&gt; +0.3821|01&gt; -0.5950i|11&gt; \ncompiled objective: Objective with 1 unique expectation values\ntotal measurements = 1\nvariables          = [a]\ntypes              = not compiled\n1.1770183545683928\n1.2067054526079515\ncompiled objective: Objective with 1 unique expectation values\ntotal measurements = 1\nvariables          = [a]\ntypes              = [&lt;class 'tequila.simulators.simulator_qulacs.BackendExpectationValueQulacs'&gt;]\n1.1770183545683928\n1.2067054526079515\n\n\n\n\nHow can I emulate a real quantum computer?\nEmulation is performed similarly to running on real devices. All you need to do is pass down the right string to the ‘device’ keyword. For qiskit, these are the same as for regular backends, but have ‘fake_’ at the beginning; I.E to emulate ‘armonk;, set device=\"fake_armonk\". For PyQuil, this is done by adding’-qvm’ to the end of the chosen string, i.e, ‘Aspen-8’ becomes device=Aspen-8-qvm'. For Cirq, only emulation is currently possible; the only string options for cirq are ‘foxtail’,‘bristlecone’,‘sycamore’, and ‘sycamore23’.\nWhen emulating, a few things about the real device will be mimicked, principally its native gate set and its connectivities. Emulation will NOT include noisy emulation by default; If you want to emulate noise, pass down the keyword noise='device'. Using this option without specifying a device will result in an error.\nBelow, we will emulate pyquil’s Aspen 8, with emulated noise. You need pyquil installed for this to work.\nadditionally: when real backends cannot be accessed, emulation will be attempted, with a warning.\n\nU = tq.gates.Ry(angle=\"a\", target=0)\nH = tq.paulis.X(0)\nE = tq.ExpectationValue(H=H, U=U)\n\n# simulate the square of the expectation value with a specific set of variables\nresult = tq.simulate(E**2, variables={\"a\":1.0}, samples=1000, backend=\"pyquil\")\nprint('sampling from pyquil yielded: ', result)\nresult = tq.simulate(E**2, variables={\"a\":1.0}, samples=1000, backend=\"pyquil\",device='Aspen-8-qvm')\nprint('sampling from pyquil while emulating Aspen-8 yielded: ', result)\nresult = tq.optimizer_scipy.minimize(E**2, initial_values={\"a\":1.0}, samples=1000,\n                                    backend='pyquil', device=\"Aspen-8-qvm\", \n                                    noise='device')\nprint('optimizing while emulating Aspen-8 with noise yielded a best energy of: ', result.energy)\n\nQVMError: Could not communicate with QVM at http://127.0.0.1:5000\n\n\n\n\nCan I compile Objectives into different backends?\nYes you can. Tequila will print a warning if this happens. Warnings can be ignored by filtering them out (see the python warnings documentation)\nIf a compiled circuit is used as input to compile then tequila will re-compile the circuit to the new backend (it it differs from the previous one)\nIf a compiled objective is used as input to compile then tequila will only compile non-compiled expectationvalues into the different backend. Already compiled expectation values will remain untouched\nNote that you need at least two different backends for the following cell to execute.\nJust change the key to whatever you have installed.\n\nbackend1 = \"qulacs\"\nbackend2 = \"cirq\"\n\nU = tq.gates.X(target=[0,1])\nprint(\"Example Circuit: \", U)\ncompiled_1 = tq.compile(U, backend=backend1)\ncompiled_2 = tq.compile(compiled_1, backend=backend2)\nprint(\"Circuit compiled to {} -&gt; \".format(backend1), compiled_1)\nprint(\"Circuit compiled to {} -&gt; \".format(backend1), compiled_1)\n\nH = tq.paulis.X(0)*tq.paulis.Y(1) + tq.paulis.X([0,1])\nprint(\"\\nmake objective with H = \", H)\nobjective = tq.ExpectationValue(H=H, U=U)\ncompiled_1 = tq.compile(objective, backend=backend1)\n\nprint(\"\\nExpectationValues of objective 1:\")\nprint(compiled_1)\n    \nobjective2 = compiled_1 + objective # Its recommended to avoid those hybrids, but in principle it works\n\nprint(\"\\nExpectationValues of partly compiled objective:\")\nprint(objective2)\n    \ncompiled_2 = tq.compile(objective2, backend=backend2)\nprint(\"\\nExpectationValues of hybdrid compiled objective:\")\nprint(compiled_2)\n\n\n\n\n\nHow do I transform Measurements into Hamiltonians?\nWe can not answer this question in general, but we can try to give a small example here.\nAssume you have a quantum circuit with \\(4\\) Qubits and you are measuring Qubit \\(0\\) and \\(2\\). You define your cost function in the following way:\n\\[\nL(AB) = A + B, \\qquad A,B \\in \\left\\{ 0,1 \\right\\}  \n\\]\nmeaning you accumulate the number of ones measured in your circuit.\nThe corresponding expectationvalue would be\n\\[\nL = \\langle \\Psi \\rvert H \\lvert \\Psi \\rangle \\qquad H = 1 - \\frac{1}{2}\\left(Z(0) + Z(1)\\right)\n\\]\nThe Hamiltonian could also be written as\n\\[\nH = 2\\lvert 11 \\rangle \\langle 11 \\rvert + \\lvert 10 \\rangle \\langle 10 \\rvert + \\lvert 01 \\rangle \\langle 01 \\rvert\n\\]\nTequila provides the convenience function tq.gates.Projector to initialize Hamiltonians like that\n\n2*tq.paulis.Projector(\"|11&gt;\") + tq.paulis.Projector(\"|01&gt;\") + tq.paulis.Projector(\"|10&gt;\")\n\nThe projector can also be initialized with more structured QubitWaveFunctions which can itself be initialized from array or string.\nHere are some examples\n\nwfn = tq.QubitWaveFunction.from_string(\"1.0*|00&gt; + 1.0*|11&gt;\")\nwfnx = tq.QubitWaveFunction.from_array(arr=[1.0, 0.0, 0.0, 1.0])\nprint(wfn == wfnx)\nwfn = wfn.normalize()\nprint(wfn)\n\nP = tq.paulis.Projector(wfn=wfn)\nprint(\"P = \", P)\n\nApart from Projector there is also KetBra which intialized more general operators like \\[\n\\lvert \\Psi \\rangle \\langle \\Phi \\rvert\n\\]\nKeep in mind that those are not hermitian.\nBut they can be split up into their hermitian and anti hermitian part where both can then be used as hamiltonians for expectationvalues.\nIf the hermitian = True key is set, the function returns the hermitian version of the operator (which is the same as the hermitian part of the old operator)\n\\[\n\\frac{1}{2}\\left(\\lvert \\Psi \\rangle \\langle \\Phi \\rvert + \\lvert \\Phi \\rangle \\langle \\Psi \\rvert \\right)\n\\]\n\nwfn1 = tq.QubitWaveFunction.from_string(\"1.0*|00&gt; + 1.0*|11&gt;\").normalize()\n\nop = tq.paulis.KetBra(bra=wfn1, ket=\"|00&gt;\")\n\nH1, H2 = op.split()\n\nprint(\"operator=\", op)\nprint(\"hermitian part      = \", H1)\nprint(\"anti-hermitian part =\", H2)\n\nH = tq.paulis.KetBra(bra=wfn1, ket=\"|00&gt;\", hermitian=True)\nprint(\"hermitianized operator = \", H)\n\n\n\nCan I do basic operations on wavefunctions and operators without quantum backends?\nIn principle yes. But keep in mind that tequila was not made for this.\nHowever, some of those operations might come in handy for debugging or small examples.\nYou can not execute circuits without a simulator since they are just abstract data types (no matrices or anything). Tequila has however its own small debug simulator backend = symbolic but there is no reason to use it if you have any other quantum backend installed.\nHamiltonians can be converted to matrices.\nWe give a few examples here\n\nwfn = tq.QubitWaveFunction.from_string(\"1.0*|0&gt; + 1.0*|1&gt;\").normalize()\nH = 1.0/numpy.sqrt(2.0)*(tq.paulis.Z(0) + tq.paulis.X(0))\nwfn2 = wfn.apply_qubitoperator(H).simplify()\n\nprint(\"|wfn&gt;  = \", wfn)\nprint(\"H      = \", H)\nprint(\"H|wfn&gt; = \", wfn2)\n\n\nwfn1 = tq.QubitWaveFunction.from_string(\"1.0*|0&gt; + 1.0*|1&gt;\").normalize()\nwfn2 = tq.QubitWaveFunction.from_string(\"1.0*|0&gt; - 1.0*|1&gt;\").normalize()\nprint(\"&lt;wfn1|wfn2&gt; = \", wfn1.inner(wfn2))\n\n\nH = 1.0/numpy.sqrt(2.0)*(tq.paulis.Z(0) + tq.paulis.X(0))\nprint(H.to_matrix())\n\n\n\nCan I import an Hamiltonian from OpenFermion?\nYes! OpernFermion is currently tequilas backend for Hamiltonians, which makes importing from it straight forward. You just need to wrap the OpenFermion QubitOperator into tequilas QubitHamiltonian.\nWe show a few examples\n\nfrom openfermion import QubitOperator\n\n# get OpenFermion QubitOperator from tequila QubitHamiltonian\nH = tq.paulis.X(0)\nof_operator = H.to_openfermion()\n\nprint(\"{} = {}\".format(type(H), H))\nprint(\"{} = {}\".format(type(of_operator), of_operator))\n\n# init tequila QubitHamiltonian with OpenFermion QubitOperator\nH = tq.QubitHamiltonian.from_openfermion(of_operator)\nprint(\"{} = {}\".format(type(H), H))\n\n# initialization from file os often read in the string form\nof_string = str(of_operator)\ntq_string = str(H)\n\nprint(of_string)\nprint(tq_string)\n\nH = tq.QubitHamiltonian.from_string(string=of_string, openfermion_format=True)\nprint(H)\nH = tq.QubitHamiltonian.from_string(string=tq_string, openfermion_format=False)\nprint(H)\n\n\n\nCan I compile into a regular function instead of one which takes dictionaries?\nNot recommended but yes. The order of the function arguments is the order you get from extract_variables\n\nU = tq.gates.Ry(angle=\"a\", target=0)\nU += tq.gates.X(power = \"b\", target=1)\nH = tq.QubitHamiltonian.from_string(\"X(0)Z(1)\")\nE = tq.ExpectationValue(H=H, U=U)\n\nf = tq.compile_to_function(E)\n\nprint(\"order is : \", E.extract_variables())\nprint(f(0.5, 1.0))\nprint(tq.simulate(E, variables={\"a\":0.5, \"b\":1.0}))\n\nIf you also want to fix the samples and other entries to your compiled objective you can build wrappers\n\ndef mywrapper(compiled_obj, samples):\n    return lambda *x: compiled_obj(*x, samples=samples)\n\nwrapped = mywrapper(f, samples=100)\n\n# don't expect same results, since samples are taken individually\nprint(wrapped(1.0, 0.5)) # always takes 100 samples\nprint(f(1.0, 0.5, samples=100)) # samples need to be given\nprint(f(1.0, 0.5, samples=1000)) # but sampling rate can be changed\nprint(f(1.0, 0.5)) # you can go back to full simulation which you cannot with the wrapped function\n\n\n\nHow do numerical gradients work?\nYes this is possible by passing for example gradient={'method':'2-point', 'stepsize': 1.e-4} to the tq.minimize function.\nThe default is a central 2-point derivative stencil where h is the stepsize: \\[\\displaystyle\n\\frac{\\partial f}{\\partial a} = \\frac{f(a+\\frac{h}{2}) - f(a-\\frac{h}{2})} {h}\n\\]\nOther methods are: 2-point-forward: Forward derivative stencil:\n\\[\\displaystyle\n\\frac{\\partial f}{\\partial a} = \\frac{f(a+h) - f(a)} {h}\n\\]\n2-point-backward: Backward derivative stencil:\n\\[\\displaystyle\n\\frac{\\partial f}{\\partial a} = \\frac{f(a) - f(a-h)} {h}\n\\]\nYou can also use your own numerical derivative stencil by passing a callable function as method.\nThe function should have the signature which is given in the example below.\nHere is an example:\n\nimport tequila as tq\n# form a simple example objective\nH = tq.paulis.X(0)\nU = tq.gates.Ry(angle=\"a\", target=0)\nE = tq.ExpectationValue(U=U, H=H)\n\n# make it more interesting by using analytical gradients for the objective\n# and numerical gradients to optimize it\n\nobjective = tq.grad(E, 'a')**2 # integer multiples of pi/2 are minima\n\n# start from the same point in all examples\ninitial_values = {'a': 2.3}\n\n\n# optimize with analytical derivatives\nresult = tq.minimize(objective=objective, method=\"bfgs\", initial_values=initial_values)\n#result.history.plot(\"energies\")\n#result.history.plot(\"gradients\")\n\n# optimize with 2-point stencil\nresult = tq.minimize(objective=E, method=\"bfgs\", gradient={'method': '2-point', 'stepsize':1.e-4}, initial_values=initial_values)\n#result.history.plot(\"energies\")\n#result.history.plot(\"gradients\")\n\n# optimize with custom stencil\n# here this is the same as the default\nimport copy\ndef mymethod(obj, angles, key, step, *args, **kwargs):\n    left = copy.deepcopy(angles)\n    left[key] += step / 2\n    right = copy.deepcopy(angles)\n    right[key] -= step / 2\n    return 1.0 / step * (obj(left, *args, **kwargs) - obj(right, *args, **kwargs))\n\nresult = tq.minimize(objective=E, method=\"bfgs\", gradient={'method': mymethod, 'stepsize':1.e-4}, initial_values=initial_values)\n#result.history.plot(\"energies\")\n#result.history.plot(\"gradients\")\n\n# optimize with a scipy method and use scipys 2-point\n# the scipy protocol will have more function evaluations and less gradient evaluations for some methods\n# the stepsize in scipy has to be passed with the `method_options` dictionary\n# with the keyword `eps`\nresult = tq.minimize(objective=E, method=\"bfgs\", gradient='2-point', method_options={'eps':1.e-4}, initial_values=initial_values)\n#result.history.plot(\"energies\")\n\n\n\n\nCan I use the numerical gradient protocols from SciPy?\nYes you can for all scipy methods by passing gradient='2-point' to tq.minimize.\nSee the scipy documentation for the stepsize and more options which can be passed with method_options dictionary where the key for the stepsize is usually eps. Note that not all scipy methods support numerical gradients, but you can always fall back to tequilas numerical gradients. See the previous cell for an example."
  },
  {
    "objectID": "Tutorials/QTensor/QTensor.html",
    "href": "Tutorials/QTensor/QTensor.html",
    "title": "QTensor",
    "section": "",
    "text": "This tutorial introduces you to the QTensor class using which you can form vectors, matrices, or tensors whose elements are tequila objectives. This class is derived from numpy.ndarray and therefore, all operations that can be performed on an ndarray object can be performed on a QTensor object."
  },
  {
    "objectID": "Tutorials/QTensor/QTensor.html#compilation-and-simulation",
    "href": "Tutorials/QTensor/QTensor.html#compilation-and-simulation",
    "title": "QTensor",
    "section": "Compilation and Simulation",
    "text": "Compilation and Simulation\nWe can compile and simulate QTensors in exactly the same way as we compile and simulate objectives.\n\n\nCode\nprint(tq.simulate(V1,variables)) \n\n\n[ 0.70710678 -0.70710678  0.96592583]\n\n\n\n\nCode\nV4 = tq.compile(V2,variables)\nprint(V4,'\\n')\nprint(V4(variables))\n\n\nQTensor of shape (2, 2) with 5 unique expectation values\ntotal measurements = 5\nvariables          = [a, b]\ntypes              = [&lt;class 'tequila.simulators.simulator_qulacs.BackendExpectationValueQulacs'&gt;] \n\n[[0.70710678 0.8660254 ]\n [0.70710678 1.        ]]"
  },
  {
    "objectID": "Tutorials/QTensor/QTensor.html#applying-transformations-on-qtensors",
    "href": "Tutorials/QTensor/QTensor.html#applying-transformations-on-qtensors",
    "title": "QTensor",
    "section": "Applying transformations on QTensors",
    "text": "Applying transformations on QTensors\nWe can apply any operation to the QTensor like we apply to a tequila objective. In the case of QTensors, the function/operation is applied element-wise. For instance\n\nV5 = V1.apply(numpy.exp)\n\n\n\nCode\nprint(V5)\nprint(tq.simulate(V5,variables))\n\n\nQTensor of shape (3,) with 3 unique expectation values\ntotal measurements = 3\nvariables          = [a, b]\ntypes              = not compiled\n[2.02811498 0.49306869 2.62721888]\n\n\n\n\nCode\nV6 = V2.apply(numpy.sin)\n\nprint(repr(V6))\n\nV6compiled  = tq.compile(V6,variables)\nprint(repr(V6compiled))\nprint(V6compiled(variables))\n\n# print(tq.simulate(V6,variables))\n\n\narray([['f([a])', 'f([b])'],\n       ['f([a])', 'f([b, a])']], dtype=object)\narray([['f([a])', 'f([b])'],\n       ['f([a])', 'f([b, a])']], dtype=object)\n[[0.64963694 0.76175998]\n [0.64963694 0.84147098]]\n\n\nYou can even define your own function and give it as input:\n\ndef my_func(x):\n    return 2*x\n\nV7 = V3.apply(my_func)\nprint(tq.simulate(V7,variables))\n\n[[[ 1.41421356 -1.41421356]\n  [ 1.93185165  1.73205081]]\n\n [[ 1.41421356  1.        ]\n  [ 1.73205081  3.43185165]]]\n\n\nSimilarly, we can apply gradient function on QTensor. grad is applied on each element of the QTensor\n\n\nCode\n# print(V6)\ndV2da = tq.grad(V2,'a')\n\nprint(repr(dV2da))\nprint(type(dV2da))\nprint(dV2da)\n\ndV2dab = tq.grad(dV2da,'b')\nprint(dV2dab)\n# compiled_dV2 = tq.compile(dV2da)\n# print(compiled_dV2(variables))\n\nprint(tq.simulate(dV2dab,variables))\n\n\narray([['f([a])', 'f([])'],\n       ['f([a])', 'f([b, a])']], dtype=object)\n&lt;class 'tequila.objective.qtensor.QTensor'&gt;\nQTensor of shape (2, 2) with 8 unique expectation values\ntotal measurements = 8\nvariables          = [a, b]\ntypes              = not compiled\nQTensor of shape (2, 2) with 6 unique expectation values\ntotal measurements = 6\nvariables          = [b, a]\ntypes              = not compiled\n[[0. 0.]\n [0. 0.]]\n\n\nWith these QTensors, we can do all the operations that we can with numpy arrays. Some of them are given below:\n\n\nCode\nV8 = V2*V2\n1V9 = numpy.dot(V1,V1)\nV10 = numpy.dot(V2,V2)\nV11 = numpy.matmul(V2,V2)\n\n2print(tq.simulate(V9,variables))\nprint(\"Type(V10): \")\nprint(type(V10))\n\n\n\n1\n\nDot product of two arrays:\n\n2\n\nSome operations on the results\n\n\n\n\n1.9330127018922196\nType(V10): \n&lt;class 'tequila.objective.qtensor.QTensor'&gt;\n\n\nHowever, there is one exception that we found. The tensordot method returns an ndarray rather than a QTensor when acting on QTensor objects. If such an issue occurs, recast as follows:\n\n\nCode\nV12 = numpy.tensordot(V3,V3)\n\nprint(list(V12.flatten()))\nprint(type(V12[0,0]))\n\n\n[f([a, b]), f([a, b]), f([a, b]), f([a, b])]\n&lt;class 'tequila.objective.objective.Objective'&gt;\n\n\n\n\nCode\nV13 = tq.QTensor(objective_list = list(V12.flatten()),shape=V12.shape)\nprint(V13)\nprint(type(V13[0,1]))\nprint(tq.simulate(V13,variables ))\n\n\nQTensor of shape (2, 2) with 6 unique expectation values\ntotal measurements = 6\nvariables          = [a, b]\ntypes              = not compiled\n&lt;class 'tequila.objective.objective.Objective'&gt;\n[[1.25       0.85662583]\n [3.08137071 3.31042685]]"
  },
  {
    "objectID": "Tutorials/CircuitCompiler/CircuitCompiler.html",
    "href": "Tutorials/CircuitCompiler/CircuitCompiler.html",
    "title": "Circuit Compiler",
    "section": "",
    "text": "A short tutorial on Gate Generators and Tequila’s compilation procedure\nIn this tutorial:\n1. we introduce the concept of a generator and how it is used in Tequila\n2. briefly describe how native compiler in Tequila works\nEvery unitary gate \\(U\\) can be written as: \\[ U = e^{-i \\frac{a}{2} G} \\]\nwhere \\(G\\) is a Hermitian matrix (i.e \\(G = G^{\\dagger}\\)) called the generator of \\(U\\) and \\(a\\) is an angle\nTequila uses above representation with the convention that\n\n\\(a\\) equals U.parameter if gate \\(U\\) is parametrized with an angle \\(a\\)\nor, if the gate \\(U\\) is not parametrized, then \\(a = \\pi\\) holds per default\n\nTo avoid confusion, \\(U\\) should be of type QGateImpl\nUsually in Tequila, one has high level access to QCircuit data structure\n\n### first import all relevant modules\n\nfrom numpy import pi\n\nimport tequila as tq\nimport tequila.circuit.gates as gates\n\nfrom tequila import QCircuit\n\nLet’s look at some simple examples for one-qubit gates\n\nRotation around Z - axis of angle \\(a\\): \\(\\hspace{0.7em} R_z(a) = e^{-i \\frac{a}{2} Z}\\)\n\\(a\\) -th power of Z: \\(\\hspace{1.5em} Z = e^{-i \\frac{\\pi}{2} (Z - 1)} \\implies Z^a = e^{-i \\frac{a \\cdot \\pi}{2} (Z - 1)} = e^{i \\frac{a \\cdot \\pi}{2}} \\cdot R_z(a \\cdot \\pi)\\)\n\n\n1qc  = QCircuit()\n2qc += gates.Rz(pi / 2, 0)\n3qc += gates.Z(0, power=0.5)\n\n# let's see how the data structure looks like\nfor U in qc.gates:  \n    G = U.generator  \n4    print(f\"Gate: {U.name} | Generator {G}\")\n\n\n1\n\ncreate a circuit object\n\n2\n\nadd a rotation Z gate with an angle of 90 deg.\n\n3\n\nadd a Z gate with a power of 0.5\n\n4\n\nprint the gates with their names and generator values\n\n\n\n\nGate: Rz | Generator +1.0000Z(0)\nGate: Z | Generator +1.0000Z(0)-1.0000\n\n\nGoing forward, a controlled - \\(U\\) gate with \\(k\\) qubits as controls is written as:\n\\[ U_c = e^{-i \\frac{a}{2} \\cdot |1 \\rangle \\langle 1 |^{\\otimes k} G} = e^{-i \\frac{a}{2} \\big(\\frac{I - Z}{2}\\big)^{\\otimes k}G} \\]\nWe can justify the above expression since if:\n\nany of the control qubits in state \\(| \\psi \\rangle\\) are in \\(|0 \\rangle\\) state, then :\n\n\\[ |1 \\rangle \\langle 1 |^{\\otimes k} G \\cdot | \\psi \\rangle = 0 \\implies U_c |\\psi \\rangle = |\\psi \\rangle \\]\n\nall the control qubits in state \\(| \\psi \\rangle = |c \\rangle |t \\rangle\\) are in \\(|1 \\rangle\\) state:\n\n\\[ |1 \\rangle \\langle 1 |^{\\otimes k} G \\cdot | \\psi \\rangle =\n   |1 \\rangle^{\\otimes k} G |t \\rangle\n   \\implies\n   U_c |\\psi \\rangle = |1 \\rangle^{\\otimes k} U |t \\rangle\n\\]\nIn Tequila:\n\n1qc = gates.Rx(pi / 2, control=0, target=1)\n\nfor U in qc.gates: \n2    G  = U.generator\n3    Gc = U.make_generator(include_controls=True)\n    \n4    print('\\nGate: ', U.name,\n          '\\n | Is controlled? ', U.is_controlled(), \n          '\\n | Generator:', G, \n          '\\n | Full Generator: ', Gc)\n\n\n1\n\nadd a conrtolled X rotation of 90 deg. with. Control qubit is the first and target the second\n\n2\n\ncollect generators parameters of each generator in circuit qc\n\n3\n\ncollect parameters of full generators in qc\n\n4\n\noutput the parameters of each gate in circuit qc\n\n\n\n\n\nGate:  Rx \n | Is controlled?  True \n | Generator: +1.0000X(1) \n | Full Generator:  +0.5000X(1)-0.5000Z(0)X(1)\n\n\n\nGates decomposition\nIn this section we explain the gates decomposition used by Tequila’s compiler.\nAs a start, for backends that do not natively support power gates (eg. qiskit), we decompose them as an equivalent up to a global phase sequence of rotations (see above for \\(Z^a\\)).\nA special case is the Hadamard gate:\n\\(\\hspace{0.5em} H = R_y(\\frac{\\pi}{4}) \\cdot Z \\cdot R_y(-\\frac{\\pi}{4}) \\implies H^a = e^{i \\frac{a \\cdot \\pi}{2}} \\cdot R_y(\\frac{\\pi}{4}) \\cdot R_z(a \\cdot \\pi) \\cdot R_y(-\\frac{\\pi}{4})\\)\n\n1qc = gates.H(target=0, power=0.5)\n\n2compiled = tq.compile(qc, backend='qiskit')\n\n\n\n\n1\n\ncreate a squared hadamard gate with a target qubit\n\n2\n\ncompile the circuit qc, containing only one hadamard gate, - this will decompose the gate as shown above to rotations and paulis\n\n\n\n\n\nPrinting the object compiled.circuit should yield a circuit like the one above.\nNote that as a part of the circuit compilation process Tequila will decompose the \\(H\\) gate a combination of parametrized rotation gates \\(R_x, R_y, R_z\\).\nA print will show the decomposed version (and not a \\(H\\) gate)\nRemember quantum circuits are written in reversed order than matrix multiplication. Also notice that the global phase is dropped in the decomposition.\nRegarding (multi) controlled gates, Tequila decompose everything in terms of \\(CNOT\\) and single qubit - rotations if a backend does not support the corresponding operation.\nLet’s start first with a controlled - \\(Z\\) rotation with \\(k\\) qubits acting as controls. All other cases will be reduced to it.\n\\[ CRz_k(a) = e^{-i \\frac{a}{2} \\big(\\frac{I - Z}{2}\\big)^{\\otimes k} Z} =\n\\prod_{m = 0}^{k} \\prod_{\\sigma \\in C_{k, m}} e^{-i \\frac{(-1)^{m} \\cdot a/2^k}{2} Z_{\\sigma(1)} Z_{\\sigma(2)} .. Z_{\\sigma(m)} Z} \\]\nThe idea is to sort the terms in \\(\\big(I - Z\\big)^{\\otimes k}\\) expansion in Gray code order. In this order, when we synthesize the individual Exponential-Pauli gates, consecutive \\(CNOT\\) gates meet and cancel out.\nFor example:\n\\[ CRz_2(a) = e^{-i \\frac{a}{2} \\big(\\frac{I - Z}{2}\\big)^{\\otimes 2}} =\ne^{-i \\frac{a/4}{2} I_0 I_1 Z} \\cdot\ne^{i \\frac{a/4}{2} Z_0 I_1 Z}  \\cdot\ne^{-i \\frac{a/4}{2} Z_0 Z_1 Z} \\cdot\ne^{i \\frac{a/4}{2} I_0 Z_1 Z} \\]\n\n1qc = gates.Rz(pi, control=[0, 1], target=2)\n2compiled = tq.compile(qc, backend='qiskit')\n\n3print(compiled.circuit)\n\n\n1\n\ncreate a parametrized rotation \\(Z\\) gate with two controls and an angle of 180 deg.\n\n2\n\ncompile the circuit - this will decompose it to a sequence of parametrized rotation \\(Z\\) gates and \\(CNOT\\) gates\n\n3\n\ndisplay the compiled circuit\n\n\n\n\n                                                                       \nq_0: ─────────────■────────────────────────────────■───────────────────\n                  │                                │                   \nq_1: ─────────────┼────────────────■───────────────┼────────────────■──\n     ┌─────────┐┌─┴─┐┌──────────┐┌─┴─┐┌─────────┐┌─┴─┐┌──────────┐┌─┴─┐\nq_2: ┤ Rz(π/4) ├┤ X ├┤ Rz(-π/4) ├┤ X ├┤ Rz(π/4) ├┤ X ├┤ Rz(-π/4) ├┤ X ├\n     └─────────┘└───┘└──────────┘└───┘└─────────┘└───┘└──────────┘└───┘\nc: 3/══════════════════════════════════════════════════════════════════\n                                                                       \n\n\n\nyou can print the object compiled.circuit with a normal print statement in your IDE like shown in the code snippet above and expect the same circuit like the one in the graphic\nNote that a print will also denote the angular parametrization of the rotations gates in each one of them. Moreover the \\(CNOT\\) gates are displayed as \\(X\\) gates, which is physically the same.\nThat was a lot at once.\nLet’s see how the decomposition of \\(e^{-i \\frac{a/4}{2} Z_0 Z_1 Z}\\) looks like.\n\n1qc = gates.ExpPauli(paulistring='Z(0)Z(1)Z(2)', angle=pi/4)\n\n2compiled = tq.compile(qc, backend='qiskit')\n\n\n1\n\ncreate a gate based of a given Pauli-String of \\(Z\\) gates parameterized with an angle of 45 deg.\n\n2\n\ncompile this gate - this will decompose it to a combination of \\(CNOT\\) gates and rotations\n\n\n\n\n\nPrinting compiled.circuitwill yield a circuit like the one above.\nThe decomposition of other terms look similar. You can write them down for yourself as a verification and see how the \\(CNOT\\) gates cancel out\nControlled - Phase gates:\n\\[ CP_k(a) = e^{i a \\big(\\frac{I - Z}{2}\\big)^{\\otimes k} \\frac{I - Z}{2}} =  \ne^{i \\frac{a}{2} \\big(\\frac{I - Z}{2}\\big)^{\\otimes k}} \\cdot e^{-i \\frac{a}{2} \\big(\\frac{I - Z}{2}\\big)^{\\otimes k} Z} =\nCP_{k-1}(\\frac{a}{2}) \\cdot CRz_k(a)\\]\nThe above equality provides a recurrent way to decompose \\(CP_k(a)\\) with base case \\(CP_0(a) = Phase(a) = e^{i\\frac{a}{2}} R_z(a)\\)\n\n1qc = gates.Phase(phase=pi, control=0, target=1)\n\n2compiled = tq.compile(qc, backend='qiskit')\n\n\n1\n\ncreate a controlled gate in qubit 1. with control qubit in 0. and a phase of 180 deg.\n\n2\n\ncompile this gate - thus decompose it to rotations and \\(CNOT\\)s\n\n\n\n\n\nPrinting compiled.circuit will output a circuit like the one above\nControlled power gates are equivalent to controlled - phase gates after a basis change. Specifically:\n\n\\(X = H Z H \\implies CX_k^a = H \\cdot CP_k(a \\cdot \\pi) \\cdot H\\)\n\\(Y = R_x(- \\frac{\\pi}{2}) \\cdot Z \\cdot R_x(\\frac{\\pi}{2}) \\implies CY_k^a = R_x(- \\frac{\\pi}{2}) \\cdot CP_k(a \\cdot \\pi) \\cdot R_x(\\frac{\\pi}{2})\\)\n\nIt is worth mentioning here that Tequila decomposes gates only when it is needed, i.e either a backend does not support a gate type or a translation between Tequila and backend object is not supported.\nFor example, a Toffoli gate in cirq backend will not be decomposed further:\n\n1qc = gates.Toffoli(0, 1, 2)\n\n2compiled = tq.compile(qc, backend='cirq')\n\n\n1\n\ncreate a Toffoli gate with controls in qubits 1 and 2 and target in qubit 2\n\n2\n\ncompile this gate - since this gate is recognized by the backend it won’t get decomposed to rotations and \\(CNOT\\) gates\n\n\n\n\n\nPrinting compiled.circuit should result in the above circuit\nBut with qiskit we have to decompose it in terms of one and two qubit gates.\n\n1qc = gates.Toffoli(0, 1, 2)\n\n2compiled = tq.compile(qc, backend='qiskit')\n\n\n\n1\n\ncreate the same Toffoli gate as before\n\n2\n\ncompile the gate with a different backend “qiskit”. Since this backend doesn’t recognize this gate it will decompose it to smaller gates\n\n\n\n\n\n\nOnce again printing the compiled.circuitwould yield the above circuits"
  },
  {
    "objectID": "Tutorials/tq-get-started/index.html",
    "href": "Tutorials/tq-get-started/index.html",
    "title": "Tequila Basics: Getting Started",
    "section": "",
    "text": "Tequila is a versatile framework for the development and execution of (variational) quantum algorithms. It provides a high-level abstraction that allows users to easily formulate, combine, and optimize complex quantum objectives.\nWith built-in support for automatic differentiation, Tequila simplifies the process of refining these objectives. The framework is compatible with both state-of-the-art quantum simulators and real quantum hardware, allowing seamless execution across platforms.\nThe following image showcases the creation of some abstract quantum functions with tequila. As you can see, the underlying functionalities are implemented at a very high-level manner, such that the required inputs for function calls are more of less straightforward mathematical expressions.\n\n\n\n\nInstallation\nIf you are on Linux or Mac, you can install tequila directly from the PyPi cloud via\npip install tequila-basic\nThis installs you the tequila package with only the absolutely necessary dependencies. Using windows as an operating systems will lead to errors here as the jax library is not supported.\nIn the future there will be a separate blog entry for windows users, meanwhile see the github readme.\nNot necessary but recommended it the installation of a capable simulator for you quantum circuits. Tequila supports the usual suspects, the by far fastest option is however the qualcs simulator that can conveniently be installed as\npip install qulacs\n\n\nDefine an Expectation Value\nAn expectation value \\(E = \\langle H \\rangle_U\\) consists of two parts: 1. a quantum circuit U (a unitary operation expressed as a sequence of unitaries) that defines the wavefunction 2. a qubit Hamiltonian H (an hermitian operator expressed in Pauli-strings)\nHere is a small example with the Hamiltonian \\[ H = X(0)X(1) + \\frac{1}{2} Z(0)Z(1) \\] and the quantum circuit\n\n\n\n\n\nimport tequila as tq\n\nH = tq.paulis.X([0,1]) + 0.5*tq.paulis.Z([0,1])\nU = tq.gates.Ry(\"a\", 0) + tq.gates.X(1,control=0)\nE = tq.ExpectationValue(H=H, U=U)\n\nNote that we have parametrized one of the gates (marked in pink), so that our expectation value becomes a function of this parameter \\[E=f(a).\\]\n\n\nEvaluate the Expectation Value\nWe can evaluate the expectation value by compiling it to a backend (a simulator or interface to some quantum hardware)\n\nf = tq.compile(E)\nevaluated = f({\"a\":1.0})\nprint(\"f(1.0) = {:+2.4f}\".format(evaluated))\n\nf(1.0) = +1.3415\n\n\nIf you have installed qulacs before then tq.compile translated the underlying objects to qulacs. Otherwise another simulator on your system, or the horrible tequila debug simulator was used. You can figure out which simulators you have installed by calling tq.show_available_simulators. When passing backend=\"qulacs\" to tq.compile you can specify which backend you want to compile to.\nIf ever in doubt if the current object is already compiled, just print it\n\nprint(\"abstract expectation value is:\")\nprint(E)\nprint(\"compiled function is:\")\nprint(f)\n\nabstract expectation value is:\nObjective with 1 unique expectation values\ntotal measurements = 2\nvariables          = [a]\ntypes              = not compiled\ncompiled function is:\nObjective with 1 unique expectation values\ntotal measurements = 2\nvariables          = [a]\ntypes              = [&lt;class 'tequila.simulators.simulator_qulacs.BackendExpectationValueQulacs'&gt;]\n\n\nThe printout states that our objects are tq.Objectives with a given number of unique expectation values. More about this now.\n\n\nDifferentiate and Manipulate Expectation Values\nThe expectation value can be used to define more complicated objects. Here are some examples:\nThe gradient \\[ \\frac{\\partial }{\\partial a } E(a) \\] with respect to variable a can be computed as\n\ndE = tq.grad(E,\"a\")\nprint(dE)\n\nObjective with 2 unique expectation values\ntotal measurements = 4\nvariables          = [a]\ntypes              = not compiled\n\n\nand the corresponding object can then be compiled and evaluated in the same way\n\ndf = tq.compile(dE)\nevaluated = df({\"a\":1.0})\nprint(\"df/da(1.0) = {:+2.4f}\".format(evaluated))\n\ndf/da(1.0) = +0.5403\n\n\nApart from differentiating we can also combine the tq.Objective objects. The final example illustrates most of these possibilities.\n\n\nExample\nLet’s create the function \\[ L(a) = \\frac{\\partial E}{\\partial a}(a) E(a)^3 + e^{-\\left(\\frac{\\partial E }{\\partial a}(a) \\right)^2} \\]\n\nL = E**3*dE +  (-dE**2).apply(tq.numpy.exp)\nl = tq.compile(L)\n\nAnd now let’s have a look how this function and it’s gradient \\(\\frac{\\partial L}{\\partial a}\\) looks like\n\n\nCode\nimport numpy\nimport matplotlib.pyplot as plt\n\n# get the abstract gradient and compile\ndL = tq.grad(L, \"a\")\ndl = tq.compile(dL)\n\nx = list(numpy.linspace(-numpy.pi, numpy.pi, 100))\ny0 = [ l({\"a\":xx}) for xx in x]\ny1 = [ dl({\"a\":xx}) for xx in x]\nplt.plot(x,y0, label=\"$L(a)$\", color=\"navy\")\nplt.plot(x,y1, label=r\"$\\frac{\\partial L}{\\partial a}(a)$\", color=\"tab:red\")\nplt.legend()\nplt.savefig(\"fig.png\")\nplt.show()"
  },
  {
    "objectID": "Tutorials/AdaptiveMethods/jupyter_notebook.html",
    "href": "Tutorials/AdaptiveMethods/jupyter_notebook.html",
    "title": "Adaptive Methods in Variational Quantum Algorithms",
    "section": "",
    "text": "This notebook explores the implementation of adaptive quantum algorithms using the Tequila library, focusing on variational quantum eigensolvers (VQE)."
  },
  {
    "objectID": "Tutorials/AdaptiveMethods/jupyter_notebook.html#introduction-and-general-overview-of-quantum-algorithms",
    "href": "Tutorials/AdaptiveMethods/jupyter_notebook.html#introduction-and-general-overview-of-quantum-algorithms",
    "title": "Adaptive Methods in Variational Quantum Algorithms",
    "section": "1.1 Introduction and general overview of quantum algorithms",
    "text": "1.1 Introduction and general overview of quantum algorithms\nIn a world where a steadily increasing number of problems can be solved by traditional computers and processors, the limits of traditional processors for specific problems have been becoming more and more apparent.\nThis especially is the case for problems related to chemistry and quantum physics, simulations require an exponentially scaling amount of computational resources and time to complete.\nIn the current era of NISQ-scale quantum computers, where no widespread quantum superiority (as in, quantum computers that solve problems faster than a traditional computer) yet exists, but is to be expected - algorithms have to conform to the limitations of the hardware. An interesting application area for these algorithms is the accurate simulation of the physical and chemical properties of atoms, molecules, and other many-body systems on a quantum level (Cerezo et al. 2021)."
  },
  {
    "objectID": "Tutorials/AdaptiveMethods/jupyter_notebook.html#tequila-as-a-tool-for-quantum-algorithm-development",
    "href": "Tutorials/AdaptiveMethods/jupyter_notebook.html#tequila-as-a-tool-for-quantum-algorithm-development",
    "title": "Adaptive Methods in Variational Quantum Algorithms",
    "section": "1.2 Tequila as a tool for quantum algorithm development",
    "text": "1.2 Tequila as a tool for quantum algorithm development\nDeveloping quantum algorithms without a toolset that allows testing them makes for a difficult task, similar to the early era of classical computing that only allowed for programs to be written on punch cards, only being able to debug them after they have been written.\nWhile there are multiple tools and libraries to help with the rapid prototyping of quantum algorithms, we focus this notebook on the library Tequila (Jakob S. Kottmann et al. 2021).\nTequila includes a broad toolset of predefined gates, functions, and utilities which allows us to rapidly test and prototype quantum algorithms. Additionally, it integrates with multiple other quantum backends, for chemical and physical computations (Jakob S. Kottmann et al. 2021), and includes methods for automatic differentiation.\nIn the following section, we introduce the basic concepts of Tequila and how it can be used to develop and simulate quantum algorithms.\n\nIn its toolset, Tequila also includes the Pauli operators, which we will list here for reference: \\[\nP(X) = \\ket{0}\\bra{1} + \\ket{1}\\bra{0} = \\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix}\n\\tag{1}\\] \\[\nP(Y) = -i\\ket{0}\\bra{1} + i\\ket{1}\\bra{0} = \\begin{pmatrix} 0 & -i \\\\ i & 0 \\end{pmatrix}\n\\tag{2}\\] \\[\nP(Z) = \\ket{0}\\bra{0} - \\ket{1}\\bra{1}  = \\begin{pmatrix} 1 & 0 \\\\ 0 & -1 \\end{pmatrix}\n\\tag{3}\\]\n\n1.2.1 Simulating a simple circuit\n\nAs an example, we define a circuit in tequila with a simple Pauli X gate, operating on qubit 0, and simulate it.\n\ncircuit = tq.gates.X(target=0)\n\n\n\n\nFigure 1: Basic circuit with a Pauli X gate on qubit 0\n\n\nWhen we simulate the circuit, the result is the wavefunction that the circuit results in, which in this case, is the probability amplitude of measuring the state \\(\\ket{1}\\) on qubit 0. It is important to note, all qubits are initialized in the 0 state, which means, that the circuit in Figure 1 essentially flips the state of qubit 0 from \\(\\ket{0}\\) to \\(\\ket{1}\\).\n\ntq.simulate(circuit)\n\n+1.0000|1&gt; \n\n\nConsequently adding another Pauli X gate to the circuit that acts on qubit 0, we can see that the probability of \\(\\ket{0}\\) on qubit 0 is now 0, as all Pauli matrices are unitary and therefore self-inverse.\n\ncircuit += tq.gates.X(target=0)\n\n\n\n\nFigure 2: Basic circuit with two Pauli X gates on qubit 0\n\n\n\ntq.simulate(circuit)\n\n+1.0000|0&gt; \n\n\n\n\n1.2.2 Multi-qubit circuits\nAdding a controlled Pauli X gate, acting on qubit 1 and controlled by qubit 0 (in between the two Pauli X gates), we get the expected result: The state \\(\\ket{01}\\) has a probability amplitude of 1.\n\ncircuit = tq.gates.X(0) +  tq.gates.X(target=1, control=0) + tq.gates.X(0)\n\n\n\n\nFigure 3: Circuit with two Pauli X gates on qubit 0 and a controlled Pauli X gate on qubit 1 controlled by qubit 0\n\n\n\ntq.simulate(circuit)\n\n+1.0000|01&gt; \n\n\nNow, let us define a more complex circuit, which consists of a Hadamard gate on qubit 0, followed by a CNOT gate with qubit 0 as the control and qubit 1 as the target. This circuit is equivalent to the base change of two qubits to the bell basis.\n\ncircuit = tq.gates.H(target=0) + tq.gates.CNOT(control=0, target=1)\n\n\n\n\nFigure 4: Circuit with a Hadamard gate on qubit 0 and a CNOT gate with qubit 0 as the control and qubit 1 as the target\n\n\n\ntq.simulate(circuit)\n\n+0.7071|00&gt; +0.7071|11&gt; \n\n\n\n\n1.2.3 Parametrized circuits\nAdditionally, we can parametrize the circuits using variables, which are used to parametrize rotation or phase shifts. The most basic rotations are all based on Pauli matrices. With \\(p \\in {X, Y, Z}\\), and \\(\\sigma{_p}\\) being the corresponding pauli matrix (see Equation 1), the Pauli rotation around \\(p\\) can be described by: \\[\nR_p(\\Phi) = e^{(-i\\frac{\\Phi}{2}\\sigma{_p})} = \\cos(\\frac{\\Phi}{2}) -i \\sin(\\frac{\\Phi}{2}\\sigma{_p})\n\\]\nTo further illustrate the Pauli rotations that we will use here, they are listed as matrices in the following:\n\\[\nR_X(\\Phi) = \\begin{pmatrix} \\cos(\\frac{\\Phi}{2}) & -i\\sin(\\frac{\\Phi}{2}) \\\\ -i\\sin(\\frac{\\Phi}{2}) & \\cos(\\frac{\\Phi}{2}) \\end{pmatrix}\n\\tag{4}\\] \\[\nR_Y(\\Phi) = \\begin{pmatrix} \\cos(\\frac{\\Phi}{2}) & -\\sin(\\frac{\\Phi}{2}) \\\\ \\sin(\\frac{\\Phi}{2}) & \\cos(\\frac{\\Phi}{2}) \\end{pmatrix}\n\\tag{5}\\] \\[\nR_Z(\\Phi) = \\begin{pmatrix} e^{-i\\frac{\\Phi}{2}} & 0 \\\\ 0 & e^{i\\frac{\\Phi}{2}} \\end{pmatrix}\n\\tag{6}\\]\nWhereas Equation 4 and Equation 5 are rotations around the x and y-axis of the Bloch sphere, and Equation 6 is a phase shift around the z-axis of the Bloch sphere, respectively.\nThe following example demonstrates this using a Pauli Y rotation based on the variable a. We set the variable a to \\(\\frac{\\pi}{2}\\), which results in the Pauli Y rotation which, when adding a Pauli Z gate is equivalent to a Hadamard gate.\n\ncircuit = tq.gates.Ry('a', 0) + tq.gates.Z(0)\ntq.simulate(circuit, {'a': pi/2})\n\n+0.7071|0&gt; -0.7071|1&gt; \n\n\n\n\n\nFigure 5: Circuit with a parametrized Y rotation and Z gate on qubit 0"
  },
  {
    "objectID": "Tutorials/AdaptiveMethods/jupyter_notebook.html#sec-adaptive-methods",
    "href": "Tutorials/AdaptiveMethods/jupyter_notebook.html#sec-adaptive-methods",
    "title": "Adaptive Methods in Variational Quantum Algorithms",
    "section": "1.3 Adaptive methods in quantum algorithms",
    "text": "1.3 Adaptive methods in quantum algorithms\nIn general, most quantum algorithms are based on optimization problems, so-called variational algorithms. Variational quantum algorithms (VQA) are based on the combination of a quantum circuit, which is parameterized by a set of variables, and a classical optimizer, which is used to find the optimal set of variables for the given problem (Cerezo et al. 2021).\nA challenge in the development of variational quantum algorithms is the choice of the quantum circuit, which is used to solve the problem. This circuit is called the ansatz, and it is the basis for the performance of the algorithm. Many different ansatzes have been proposed, including the unitary coupled cluster ansatz, inherent to many-body physics, and the choice of the ansatz itself is based on the problem that is to be solved (Cerezo et al. 2021).\nInstead of using a fixed ansatz as a basis for the VQA, it is also possible to construct the circuit ansatz adaptively, based on the optimization problem we work on. We also call this the variable structure ansatz, as the structure of the ansatz is variable and depends on the problem (Cerezo et al. 2021)."
  },
  {
    "objectID": "Tutorials/AdaptiveMethods/jupyter_notebook.html#introduction-of-vqes",
    "href": "Tutorials/AdaptiveMethods/jupyter_notebook.html#introduction-of-vqes",
    "title": "Adaptive Methods in Variational Quantum Algorithms",
    "section": "2.1 Introduction of VQEs",
    "text": "2.1 Introduction of VQEs\nBefore detailing and implementing an adaptive variational quantum algorithm, we first introduce the concept of one of the first variational quantum algorithms, the Variational Quantum Eigensolver (VQE) proposed by Peruzzo et al. (2014).\nAs already briefly mentioned in Section 1.3, variational quantum algorithms are based on the concept of a combination of the capabilities of a quantum computer, which can simulate quantum states (for example, the ground energy of a molecule), and the capabilities of a classical computer, which can optimize the parameters of a quantum circuit (Cerezo et al. 2021).\nNotably, we refer to the QVE (Quantum Variational Eigensolver) in Peruzzo et al. (2014) as VQE, as it is more commonly known as VQE in the literature."
  },
  {
    "objectID": "Tutorials/AdaptiveMethods/jupyter_notebook.html#sec-basic-vqe",
    "href": "Tutorials/AdaptiveMethods/jupyter_notebook.html#sec-basic-vqe",
    "title": "Adaptive Methods in Variational Quantum Algorithms",
    "section": "2.2 Implementing a basic VQE",
    "text": "2.2 Implementing a basic VQE\nIn this section, we will implement a basic VQE using Tequila, which will serve as a basis for the implementation of an adaptive VQE in the next section. In the following block, we calculate the ground state energy of \\({He}_2\\), using the fixed structure ansatz UpCCGSD, a generalized unitary coupled cluster ansatz, that contains single and double excitations (Cerezo et al. 2021).\n\ngeometry = \"He 0.0 0.0 0.0\\nHe 0.0 0.0 0.5\"\nmolecule = tq.Molecule(geometry=geometry, basis_set='6-31g')\nU = molecule.make_upccgsd_ansatz()\nH = molecule.make_hamiltonian()\nprint(f\"{U.depth = }\")\n\nU.depth = 18\n\n\nInspecting the depth of the circuit, we can see that the depth of the circuit is 18. As every additional gate in the circuit introduces more noise, it is important to consider the circuit depth for current quantum computers.\nNonetheless, we can simulate the circuit and calculate the ground state energy of the Helium molecule.\n\n\n1E = tq.ExpectationValue(H=H, U=U)\n2result = tq.minimize(objective=E, silent=True, method='adam', maxiter=80)\n\nresult.history.plot('energies')\nprint(f\"{result.energy = }\")\n\n\n1\n\nDefine the abstract expectation value, that we will use to optimize the circuit.\n\n2\n\nMinimize the expectation value, which results in the optimal parameters for the circuit.\n\n\n\n\n\n\n\nresult.energy = -4.612504938737554\n\n\nAfter optimizing the abstract expectation value E, simulating the quantum computer computations, and optimizing the parameters of the circuit using the classical optimization method Adam, we get our result of the ground state energy, which we can use to evaluate the result of our adaptive VQE in the next section."
  },
  {
    "objectID": "Tutorials/AdaptiveMethods/jupyter_notebook.html#implementing-a-basic-adaptive-vqe",
    "href": "Tutorials/AdaptiveMethods/jupyter_notebook.html#implementing-a-basic-adaptive-vqe",
    "title": "Adaptive Methods in Variational Quantum Algorithms",
    "section": "3.1 Implementing a basic adaptive VQE",
    "text": "3.1 Implementing a basic adaptive VQE\nTequila includes a implementation of an AdaptiveSolver, that takes an operator_pool, a pre and post-unitary, as well as a Hamiltonian to minimize. For illustrative purposes, we will implement our version of an adaptive solver, based on the works of Jakob S. Kottmann, Anand, and Aspuru-Guzik (2021).\nAs we have mentioned before, the adaptive method is based on the concept of a variable structure ansatz. We construct this ansatz iteratively, in the following simplified steps:\n\nCalculate the gradients for all operators in the operator pool.\nChoose the operator which yields the highest gradient and add it to the circuit.\nMinimize the abstract expectation value for the updated circuit, optimizing the parameters (see Section 1.2.3) of the updated circuit (Jakob S. Kottmann, Anand, and Aspuru-Guzik 2021).\n\nThe iteration is usually stopped with stopping criteria based on the gradients of the latest iteration falling under a certain defined threshold. Note that we will explicitly only iterate 4 times, to allow for a clear illustration.\n\nimport tequila as tq\nimport numpy as np\n\ndef calculate_gradients(operator_pool, circuit, H, variables):\n    \"\"\"\n    calculate gradients for a given circuit and operator pool\n    :param operator_pool: list of pauli strings\n    :param circuit: tequila circuit\n    :param H: tequila hamiltonian\n    :param variables: dictionary of variables\n    :return: list of gradients\n    \"\"\"\n\n    gradients = []\n    for operator in operator_pool:\n        if 'screen_angle' in circuit.extract_variables():\n            raise Exception(\"variable screen_angle already exists\")\n        gate = tq.gates.Rp(paulistring=operator, angle='screen_angle')\n        test_circuit = circuit + gate\n        E = tq.ExpectationValue(U=test_circuit, H=H)\n        objective = tq.grad(E, 'screen_angle')\n        # expand our variables with our screen_angle\n        new_variables = {**variables}\n        new_variables['screen_angle'] = 0.0\n        gradient = tq.simulate(objective, variables=new_variables)\n        gradients.append(gradient)\n    return gradients\n\nExtending the example implementation, we implement a simplified iteration step in the following, that first uses calculate_gradients from before to calculate a list of gradients for the operator pool. Following the gradient calculation, we choose the operator in the pool with the highest gradient and append the corresponding Pauli rotation Rp to the circuit. Finally, we construct the expectation value for the updated circuit and calculate the new, minimized variable mappings.\n\ndef do_iteration(operator_pool, circuit, H, variables, iteration):\n    gradients = calculate_gradients(\n        operator_pool=operator_pool,\n        circuit=circuit,\n        H=H,\n        variables=variables)\n    max_index = np.argmax(np.abs(gradients))\n\n1    gate = tq.gates.Rp(\n        paulistring=operator_pool[max_index],\n        angle=f'iter_{iteration}'\n        )\n\n2    circuit += gate\n\n    E = tq.ExpectationValue(U=circuit, H=H)\n3    result = tq.minimize(E, silent=True)\n    variables.update(result.variables)\n    return gradients\n\n\n1\n\nCreate gate with maximum gradient\n\n2\n\nAdd gate to circuit\n\n3\n\nMinimize and update the optimize variable values\n\n\n\n\nIn the following section, we illustrate an example of the adaptive circuit construction, using the implementation of the iteration and gradient calculation function from before."
  },
  {
    "objectID": "Tutorials/AdaptiveMethods/jupyter_notebook.html#running-example-iterations",
    "href": "Tutorials/AdaptiveMethods/jupyter_notebook.html#running-example-iterations",
    "title": "Adaptive Methods in Variational Quantum Algorithms",
    "section": "3.2 Running example iterations",
    "text": "3.2 Running example iterations\nFor initialization, we start with defining an example Hamiltonian and initial circuit, which allows us to show the adaptive circuit construction. Therefore, we will use a simple Hamiltonian, which consists of a linear combination of Pauli strings.\nWe define our Hamiltonian H that we want to minimize as follows: \\[\nH = 2\\cdot(X(0) \\cdot X(1) + Y(0) \\cdot Y(1)) + 10 \\cdot (X(3) \\cdot X(5) + Y(3) \\cdot Y(5))\n\\]\nAdditionally, we set up an initial circuit with two Pauli X gates, which will initialize the state with 1 on qubits 0 and 2. This is technically not necessary, as the Pauli rotations can reach this state as well: the rotation with Pauli XY \\(e^{-i\\frac{\\Phi}{2}XY}\\) with \\(\\Phi = \\pi\\) also transforms the state \\(\\ket{00}\\) to state \\(\\ket{11}\\). In the interest of reducing the size of our operator pool, we use the initial state in the following example.\n\n1circuit = tq.gates.X(0) + tq.gates.X(3)\ncircuit.n_qubits = 6\nH = 2*(tq.paulis.X(0) * tq.paulis.X(1) + tq.paulis.Y(0) * tq.paulis.Y(1)) \\\n2    + 10*(tq.paulis.X(3) * tq.paulis.X(5) + tq.paulis.Y(3) * tq.paulis.Y(5))\n\n3operator_strings = [\"X(0)Y(1)\", \"X(3)Y(5)\"]\noperator_pool = [ \n    tq.PauliString.from_string(operator) for operator in operator_strings\n    ]\n\nvariables = {}\ncircuits = []\ngradients = []\nenergies = []\n\n\n4M = H.to_matrix()\neigen_values, eigen_vectors = np.linalg.eigh(M)\nenergy_0 = eigen_values[0]\nprint(f\"{energy_0 = }\")\nwfn_0 = tq.QubitWaveFunction(eigen_vectors[:,0])\n\n\n1\n\nDefine circuit\n\n2\n\nDefine Hamiltonian H\n\n3\n\nDefine operator pool\n\n4\n\nCalculate eigenvalues and eigenvectors of H, to calculate the ground state energy of the system.\n\n\n\n\nenergy_0 = -24.000000000000007\n\n\nNotably, as this is a simple Hamiltonian we can calculate its ground state energy, to compare it to the result of our implementation. As we show over four iterations, the resulting energy converges to the previously calculated ground state energy. Before iterating, we have a look at the initial state: The circuit has no parametrized gates yet, so the optimizer is yet unable to minimize any values.\n\nE = tq.ExpectationValue(U=circuit, H=H)\n\nresult = tq.minimize(E, silent=True)\nprint(f\"{result.energy = }\")\n\nresult.energy = 0.0\n\n\nFinally, the following code block performs 4 iterations of the adaptive solving we implemented above.\n\nfor i in range(4):\n    last_gradients = do_iteration(operator_pool, circuit, H, variables, i)\n\n    gradients.append(np.linalg.norm(last_gradients))\n    circuits.append(deepcopy(circuit))\n\n    E = tq.ExpectationValue(U=circuit, H=H)\n    result = tq.minimize(E, silent=True)\n    energies.append(result.energy)\n\n\n\n\n\n\nFigure 6: Energies and gradients over iterations\n\n\n\n\n\n\n\n\n\n\n\n(a) Initial circuit\n\n\n\n\n\n\n\n(b) Iteration 1\n\n\n\n\n\n\n\n(c) Iteration 2\n\n\n\n\n\n\n\n(d) Iteration 3\n\n\n\n\n\n\n\n(e) Iteration 4\n\n\n\n\nFigure 7: Basic adaptive circuit after iterations\n\n\nInspecting the energies and gradients in Figure 6, we see that the adaptively constructed circuit in this example allows the optimization to converge close to our target energy \\(E_0 \\approx -24\\). The gradients are highest in the first iteration, as we greedily choose the highest gradient available. Furthermore, Figure 7 illustrates the iterative construction of the circuit, where each iteration step, adds one rotation from the operator pool."
  },
  {
    "objectID": "Tutorials/AdaptiveMethods/jupyter_notebook.html#sec-adapt-vqe",
    "href": "Tutorials/AdaptiveMethods/jupyter_notebook.html#sec-adapt-vqe",
    "title": "Adaptive Methods in Variational Quantum Algorithms",
    "section": "3.3 Application of ADAPT-VQE in Tequila",
    "text": "3.3 Application of ADAPT-VQE in Tequila\nFollowing the example implementation of an adaptive solver, we will apply this concept to an adaptive VQE, using the implementation provided by the Tequila library (Jakob S. Kottmann et al. 2021). Like in Section 2.2, we will first create a Helium atom, of which we want to find the ground state energy. Moreover, we will use the utility of Tequila to infer the Hamiltonian that represents the energy of a Helium molecule.\n\ngeometry = \"He 0.0 0.0 0.0\\nHe 0.0 0.0 0.5\"\nmolecule = tq.Molecule(geometry=geometry, basis_set='6-31g')\nH = molecule.make_hamiltonian()\n\nSubsequently, we would define the ansatz that we use as starting point to optimize its parameters. In contrast to the implementation with a fixed ansatz, we will proceed to use the adaptive solver to construct the variable structure ansatz. For this, we first use Tequila’s MolecularPool, to infer the operators that we use to construct the ansatz. It is important to note, that the MolecularPool chooses Fermionic operators from existing ansatzes. In this case, we use a variant of the unitary-coupled cluster ansatz UpCCGSD (as used before, for the fixed-structure ansatz VQE), which is a common ansatz for many-body problems (Jakob S. Kottmann et al. 2021; Cerezo et al. 2021).\nAfter defining the operator pool, we construct the adaptive solver using Adapt, which we initialize with the corresponding Hamiltonian of the system, our operator pool, as well as arguments for the optimizer. We explicitly set the optimizer to a local gradient-descent-based optimizer Adam to ensure that the optimizer converges, as the default optimizer BFGS did not converge for this problem in our tests.\n\n1operator_pool = tq.adapt.MolecularPool(molecule, \"UpCCGSD\")\n2solver = tq.adapt.Adapt(\n    H=H, \n    Upre=molecule.prepare_reference(),\n    operator_pool=operator_pool,\n    optimizer_args={\n        'method': 'Adam',\n        'silent': True\n        },\n    silent=True\n    )\nresult = solver() \n3final_circuit = molecule.prepare_reference() + result.U\nprint(f\"{final_circuit.depth = }\") \nprint(f\"{result.energy = }\") \n\n\n1\n\nDefine operator pool, based on fermionic rotations from the UpCCSD ansatz.\n\n2\n\nConstruct the adaptive solver\n\n3\n\nConstruct the final circuit, which also contains the circuit preparation for the given molecule.\n\n\n\n\nfinal_circuit.depth = 9\nresult.energy = -4.612510649776234\n\n\nThe execution results above illustrate a potential advantage of adaptive circuit construction, as the circuit depth of the final circuit (Figure 9) is smaller than the same optimization executed with a static structure ansatz (see Section 2.2).\n\n\n\n\n\nFigure 8: Final minimized energies of each iteration\n\n\n\n\n\n\n\nFigure 9: Final circuit of the ADAPT-VQE\n\n\nThe decreasing final energy of the molecule with the iterations of the adaptive solver is illustrated by Figure 8, where we also observe, that no meaningful progress is being made starting at iteration 3.\nComparing the resulting energy yields that the final energy is similar to the energy reached before, where we can observe that the energies are very similar, with the fixed-ansatz VQE reaching \\(E_{fixed} \\approx -4.61250\\), whereas the adaptive method reached \\(E_{adaptive} \\approx -4.61251\\)."
  },
  {
    "objectID": "tutorials.html",
    "href": "tutorials.html",
    "title": "Tutorials",
    "section": "",
    "text": "Tequila Basics: Getting Started\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nNov 3, 2022\n\n\nJakob Kottmann\n\n\n\n\n\n\n  \n\n\n\n\nTequila Basics: Circuits\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nNov 4, 2022\n\n\nJakob Kottmann\n\n\n\n\n\n\n  \n\n\n\n\nCircuit Compiler\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nMay 20, 2024\n\n\nGeorgios Tsilimigkounakis, Adapted by Ram Mosco\n\n\n\n\n\n\n  \n\n\n\n\nBraket Module\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nJun 21, 2024\n\n\nFrancesco Scala, Adapted by Ram Mosco\n\n\n\n\n\n\n  \n\n\n\n\nOpenQASMConversions\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nJun 21, 2024\n\n\nClaudia Zendejas-Morales, Adapted by Elisabeth Welizky\n\n\n\n\n\n\n  \n\n\n\n\nQTensor\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nJun 21, 2024\n\n\nGaurav Saxena, Adapted by Elisabeth Welizky\n\n\n\n\n\n\n  \n\n\n\n\nOptimizer\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nJul 5, 2024\n\n\nSumner Alperin, Adapted by Thuy Truong\n\n\n\n\n\n\n  \n\n\n\n\nKrylov Method with Tequila\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nJul 12, 2024\n\n\nFrancesco Scala, Adapted by Thuy Truong\n\n\n\n\n\n\n  \n\n\n\n\nAdaptive Methods in Variational Quantum Algorithms\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nAug 15, 2024\n\n\nKorbinian Stein, Adapted by Thuy Truong\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "Molecular Quantum Circuits\n\n\n\n\n\n\n\ntequila\n\n\ncode\n\n\nchemistry\n\n\n\n\n\n\n\n\n\n\n\nSep 12, 2024\n\n\nJakob Kottmann\n\n\n\n\n\n\n  \n\n\n\n\nBuilding a Variational Quantum Classifier\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nJul 19, 2024\n\n\nSimon Reichert, Adapted by Thuy Truong\n\n\n\n\n\n\n  \n\n\n\n\nElectronic Hamiltonians\n\n\n\n\n\n\n\nchemistry\n\n\n\n\n\n\n\n\n\n\n\nNov 10, 2022\n\n\nJakob Kottmann\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Researches/electronic-hamiltonians/index.html",
    "href": "Researches/electronic-hamiltonians/index.html",
    "title": "Electronic Hamiltonians",
    "section": "",
    "text": "What are electronic Hamiltonians and how can we construct them?\n\n\n\n\n\nFirst Quantized Formulation:\n\nHamiltonian is directly defined from the nuclear coordinates and charges\n\nSpin and Fermionic anti-symmetry need to be imposed on the wavefunctions\n\nParticle number is fixed\n\nExplicit computation of matrix representation unfeasible without approximations\n\nSecond Quantized Formulation:\n\nNeeds a set of orbitals to be constructed\n\nSpin and Fermionic anti-symmetry included in Hamiltonian\n\nParticle number is not fixed\n\nExplicit construction in finite basis feasible\n\n\n\n\nThe word molecule often stands for \\(N_\\text{e}\\) electrons with coordinates \\(r_k\\) moving in a potential created by atomic nuclei - assumed to be fixed point-charges with coordinates \\(R_A\\) and charges \\(N_A\\). The electronic Hamiltonian is then the operator expressing the interactions of the electrons in that potential \\[H =  \\sum_{k}^{N_\\text{e}} h\\left(r_k\\right) + \\sum_{k}^{N_\\text{e}} \\sum_{l&lt;k} g\\left(r_k,r_l\\right) + V_\\text{nn}.\\] We write it three parts and a constant, which are in atomic units:\n\nOne-body interactions: \\[ h(r) = T(r) + V_\\text{ne}(r) \\] including the kinetic energy of the electrons and the Coulombic attraction between electrons and nuclear charges\n\\[T(r) = -\\frac{1}{2} {\\nabla^2_{r}}, \\quad V_{\\text{ne}}(r) = \\sum_{A \\in \\text{nuclei}} \\frac{N_A}{\\|r-R_A\\|}.\\]\n\nTwo-body interactions: the Coulombic repulsion between individual electrons\n\\[g(r_k,r_l) = \\frac{1}{\\| r_k - r_l \\|}\\]\n\nA constant term: Coulombic repulsion between the nuclear charges\n\\[V_\\text{nn} = \\sum_{A&lt;B} \\frac{N_A N_B}{\\|R_A - R_b\\|} \\]\nwhich is just a constant for a given molecular structure.\n\nWith this, we have fully defined it, and the lowest eigenvalue of this differential operator gives the corresponding ground state energy. To ensure that the ground state describes a multi-electron system, we must impose restrictions on the wavefunction - Fermions have a spin and anti-symmetric permutation symmetry.\n\n\nAs we are describing electrons, they need to obey fermionic antisymmetry. In first quantization (or real-space formulation), the Hamiltonian does not have that property included, so without restrictions on the wavefunctions, it only describes \\(N_\\text{e}\\) negatively charged point-particles without any permutation symmetries or spin.\nWe can add spin conveniently by defining a single-electron wavefunction as a so called spin-orbital: A three-dimensional function \\(\\psi(r) \\in \\mathcal{L}^2(\\mathbb{R}^3)\\), describing the electron in spatial space, augmented with a spin-state \\[\\lvert\\sigma\\rangle \\in \\text{Span}\\left\\{\\lvert \\uparrow \\rangle, \\lvert \\downarrow \\rangle\\right\\} \\equiv \\mathbb{C}^2.\\] If the set of spatial orbitals \\(\\psi_k\\) forms a complete basis in \\(\\mathcal{L}^2(\\mathbb{R}^3)\\), we have an exact description of the electron. A convenient notation is to express the spin component as a function of a spin-coordinate \\(s\\left\\{-1,1\\right\\}\\) and combine spin coordinate \\(s\\) and spatial coordinate \\(r\\) to \\(x=(r,s)\\). In a given set of \\(M\\) spatial orbital, an arbitrary one electron wavefunction can then be expressed as \\[\n\\Psi(x) = \\sum_k^{2M} c_k \\phi(x) = \\sum_{l}^{M} \\psi_{l}(r)\\otimes \\left( c_{2l}\\lvert \\uparrow \\rangle + c_{2l+1} \\lvert \\downarrow \\rangle \\right)\n\\] with the spin-orbitals defined as \\[\n\\phi_k = \\psi_{\\lfloor{i/2}\\rfloor} \\otimes \\lvert \\sigma(k) \\rangle,\\; \\sigma_k=\\begin{cases} \\lvert \\uparrow \\rangle,\\; k \\text{ is even}  \\\\  \\lvert \\uparrow \\rangle,\\; k \\text{ is odd}  \\end{cases}.  \n\\] Many electron functions can then be constructed as linear combinations of anti-symmetric products of single electron functions (so called Slater-Determinants). \\[\n\\Psi\\left(x_1, \\dots, x_{N_\\text{e}}\\right) = \\sum_{m} d_m \\det\\left(\\phi_{m_1},\\dots, \\phi_{m_{N_\\text{e}}}\\right) \\\n\\]\n\n\n\n\nUsing second quantized language, we can significantly simplify the treatment of electron spin and anti-symmetry with the help of abstract field operators \\(\\hat{\\phi}^\\dagger(x)\\) (\\(\\hat{\\phi}(x)\\)) that create (or annihilate) electrons at spin-position \\(x\\). With them, we can formaly write the electronic Hamiltonian as \\[H = \\int \\hat{\\phi}^\\dagger(x) h(x) \\hat{\\phi}(x) \\operatorname{x}x + \\int \\int \\hat{\\phi}^\\dagger(x)\\hat{\\phi}^\\dagger(y) g(x,y) \\hat{\\phi}(x)\\hat{\\phi}(y) \\operatorname{d}x\\operatorname{d}y+ V_\\text{nn}.\\] The wavefunction is now only required to carry information about electron occupancies at all points in space. When acting on the wavefunction, the one-body part of the operator is first annihilating an electron at point \\(x\\). If no electron is present, this will lead to an energy contribution of zero and otherwise invoke \\(h(x)\\), followed by the restoration of the electron by the creation operator. The two-body part acts in the same way and the integrals ensure that we probe all points in space. In practice, this is unfeasible, and it is convenient to introduce a finite basis in the form of spin orbitals to expand the field operators as \\[\\hat{\\phi}(x) = \\sum_k \\phi(x)_k a_k, \\] leading to the more prominent form of the second quantized Hamiltonian \\[\nH = \\sum_{ij} \\tilde{h}^i{j}a^\\dagger_i a_j \\sum_{ijkl} \\tilde{g}^{ij}_{kl} a^\\dagger_i a^\\dagger_j a_l a_k + V_\\text{nn}.\n\\] The new discretized operators \\(a^\\dagger_k\\) (\\(a_k\\)) are now creating (annihilating) an electron in the spin orbital \\(\\phi_k\\) and the tensors \\(\\tilde{h}^{i}_{j}\\) and \\(\\tilde{g}^{ij}_{kl}\\) are integrals over the one- and two-body operators and the spin orbitals. The one-body integrals are then given by \\[\n\\tilde{h}^{i}_{j} = h^{\\lfloor{i/2}\\rfloor}_{\\lfloor{j/2}\\rfloor} \\langle \\sigma_i \\vert \\sigma_j  \\rangle\n\\] with the spatial part \\[ h^{k}_{l} =  \\langle k \\rvert h \\lvert l \\rangle \\equiv \\int \\psi_k^*(r) h(r) \\psi_l(r) \\operatorname{d}r. \\] In the same way, the spatial part of the two-body integrals is given by \\[ g^{ij}_{kl} = \\langle i j \\rvert g_{12}  \\lvert k l \\rangle \\equiv \\int \\int \\psi_i^*(r_1) \\psi_j^*(r_2) \\frac{1}{|r_1-r_2|} \\psi_k(r_1) \\psi_l(r_2) \\operatorname{d}r_1 \\operatorname{d}r_2.  \\] Note that there exist three different short notations \\[ \\langle ij\\vert kl \\rangle \\equiv \\left(ik \\vert jl \\right) \\equiv \\left[ ij  \\vert lk \\right]  \\] usually referred to as Dirac (physicist, 1212), Mulliken (chemist, 1122) and openfermion (google, 1221) notations. Depending on the convention used, the meaning of the indices \\(g^{ij}_{kl}\\) changes. This is a bit inconvenient, but we can’t change it anymore. Most quantum chemistry packages (pyscf, psi4) use the Mulliken convention, some quantum computing packages adopted the google convention, and the Dirac convention is often found in articles. In the tequila package we tried to automatize most of it away for user convenience. Here is a small example on how to get the integrals using tequila (with either pyscf or psi4 in the back):\n\nimport tequila as tq\n\ngeometry = \"Be 0.0 0.0 0.0\"\nmol = tq.Molecule(geometry=geometry, basis_set=\"6-31G\")\n\nc,h,g = mol.get_integrals(two_body_oderings=\"mulliken\")\n\nfor most applications, the integrals are however processed automatically in the back.\n\n\n\nLet’s look at first and second quantized Hamiltonians and wavefunctions in an explicit example: The Helium atom (charge \\(N_\\text{He}=2\\) and nuclear coordinate \\(R_\\text{He} = (0,0,0)\\)) in a basis of two spatial orbitals \\(\\left\\{\\psi_0, \\psi_1 \\right\\}\\).\n\n\nFor the neutral electron with 2 electrons, the Hamiltonian is:\n\\[\nH(r_1,r_2) = -\\frac{\\nabla^2}{2} - \\frac{\\nabla^2}{2} - \\frac{2}{\\|r_1\\|} - \\frac{2}{\\|r_2\\|} + \\frac{1}{\\|r_1 - r_2 \\|}  \n\\] and a general two-electron Slater-determinant is written as\n\\[\n\\det\\left(\\phi_k,\\phi_l\\right) = \\frac{1}{\\sqrt{2}} \\left( \\phi_k(r_1) \\phi_l(r_2) - \\phi_l(r_1) \\phi_k(r_2) \\right).\n\\]\nAll possible Slater-determinants in the given basis:\n\nclosed-shell singlets (both electrons in the same spatial orbital):\n\\[\n\\det\\left(\\phi_0 \\phi_1 \\right) = \\frac{1}{\\sqrt{2}}\\psi_0(r_1) \\psi_0(r_1) \\otimes \\left( \\lvert \\downarrow \\uparrow \\rangle - \\lvert \\uparrow \\downarrow \\rangle \\right)\n\\] \\[\n\\det\\left(\\phi_3 \\phi_4\\right) = \\frac{1}{\\sqrt{2}}\\psi_1(r_1) \\psi_1(r_1) \\otimes \\left( \\lvert \\downarrow \\uparrow \\rangle - \\lvert \\uparrow \\downarrow \\rangle \\right)\n\\]\nopen-shell polarized triplets:\n\\[\n\\det\\left(\\phi_1 \\phi_3\\right) = \\left(\\psi_0(r_1) \\psi_1(r_2) + \\psi_1(r_1) \\psi_0(r_2)\\right) \\otimes \\left( \\lvert \\uparrow \\uparrow \\rangle  \\right)\n\\]\n\n\\[\n\\det\\left(\\phi_2 \\phi_4\\right) = \\left(\\psi_0(r_1) \\psi_1(r_2) + \\psi_1(r_1) \\psi_0(r_2)\\right) \\otimes \\left( \\lvert \\downarrow \\downarrow \\rangle  \\right)\n\\]\n\nopen-shell (with broken spin symmetry - recombinable into non-polarized triplet state and one additional singlet state):\n\\[\n\\det\\left(\\phi_0\\phi_2\\right) = \\frac{1}{\\sqrt{2}}\\left(\\phi_0(x_1) \\phi_2(x_2) - \\phi_2(x_1) \\phi_0(x2) \\right)\n\\] \\[\n\\det\\left(\\phi_1\\phi_3\\right) = \\frac{1}{\\sqrt{2}}\\left(\\phi_1(x_1) \\phi_3(x_2) - \\phi_3(x_1) \\phi_1(x2) \\right)\n\\]\n\nA general two-electron wavefunction can then be written as a linear combination of those 6 Determinants (note however, that the different spin-symmetries, i.e. the triplet and the two singlets, usually don’t mix) \\[\n\\Psi(x_1,x_2) = \\sum_{i&lt;j\\in\\left\\{0,1\\right\\}} d_{ij} \\det\\left(\\phi_i\\phi_j\\right)\n\\] Note how this always denotes a two-electron wavefunction. The electron number directly enters the definition of the first quantized Hamiltonian and therefore defines the space onto which the Hamiltonian acts.\nFor numerical procedures it is often necessary to directly express the Hamiltonian in the given basis. Here this would mean to compute all matrix elements \\[\nH_{ij} = \\langle \\det\\left(\\phi_m\\phi_n\\right)\\rvert H \\lvert \\det\\left(\\phi_k\\phi_l\\right) \\rangle.\n\\] In this two electron example this is no problem, the task of computing all matrix elements will however become unfeasible with growing electron number due to the growth of possible determinants. Explicit computation of the Hamiltonian in the given basis is usually only performed within further approximations - e.g. truncated configuration interaction methods that only include slater determinants that differ in a specific number of orbitals from a given reference determinant.\n\n\n\nIn second quantization it is sufficient to compute the one- and two-body integrals given above to define the Hamiltonian. They grow with the fourth power of the basis size rendering the task always feasible.\nA general wavefunction can be constructed from all possible linear combinations of electronic configurations in the given spin-orbital basis - denoted by so called occupation vectors representing which spin orbitals are occupied and not. In this case we have 4 spin orbitals then therefore \\(2^4=16\\) different configurations:\n\nvacuum\n\\[\\lvert 0000 \\rangle\\]\nsingle electron\n\\[\n\\lvert 1000 \\rangle, \\vert 0100 \\rangle, \\lvert 0010 \\rangle, \\lvert 0001\\rangle,\n\\]\ntwo electrons\n\\[\n\\lvert 1100 \\rangle, \\lvert 0011 \\rangle, \\lvert 1001 \\rangle, \\lvert 0110 \\rangle, \\lvert 1010\\rangle, \\lvert 0101 \\rangle,\n\\]\nthree electrons\n\\[\n\\lvert 1110 \\rangle, \\lvert 1101 \\rangle, \\lvert 1011 \\rangle, \\lvert 0111 \\rangle,\n\\]\nfour-electrons\n\\[\n\\lvert 1111 \\rangle.\n\\]\n\nA general wavefunction is then given by\n\\[\\lvert \\Psi \\rangle = \\sum_{k=0}^{16} c_k \\lvert \\text{binary}(k) \\rangle.\\]"
  },
  {
    "objectID": "Researches/electronic-hamiltonians/index.html#tldr",
    "href": "Researches/electronic-hamiltonians/index.html#tldr",
    "title": "Electronic Hamiltonians",
    "section": "",
    "text": "First Quantized Formulation:\n\nHamiltonian is directly defined from the nuclear coordinates and charges\n\nSpin and Fermionic anti-symmetry need to be imposed on the wavefunctions\n\nParticle number is fixed\n\nExplicit computation of matrix representation unfeasible without approximations\n\nSecond Quantized Formulation:\n\nNeeds a set of orbitals to be constructed\n\nSpin and Fermionic anti-symmetry included in Hamiltonian\n\nParticle number is not fixed\n\nExplicit construction in finite basis feasible"
  },
  {
    "objectID": "Researches/electronic-hamiltonians/index.html#first-quantized-formulation",
    "href": "Researches/electronic-hamiltonians/index.html#first-quantized-formulation",
    "title": "Electronic Hamiltonians",
    "section": "",
    "text": "The word molecule often stands for \\(N_\\text{e}\\) electrons with coordinates \\(r_k\\) moving in a potential created by atomic nuclei - assumed to be fixed point-charges with coordinates \\(R_A\\) and charges \\(N_A\\). The electronic Hamiltonian is then the operator expressing the interactions of the electrons in that potential \\[H =  \\sum_{k}^{N_\\text{e}} h\\left(r_k\\right) + \\sum_{k}^{N_\\text{e}} \\sum_{l&lt;k} g\\left(r_k,r_l\\right) + V_\\text{nn}.\\] We write it three parts and a constant, which are in atomic units:\n\nOne-body interactions: \\[ h(r) = T(r) + V_\\text{ne}(r) \\] including the kinetic energy of the electrons and the Coulombic attraction between electrons and nuclear charges\n\\[T(r) = -\\frac{1}{2} {\\nabla^2_{r}}, \\quad V_{\\text{ne}}(r) = \\sum_{A \\in \\text{nuclei}} \\frac{N_A}{\\|r-R_A\\|}.\\]\n\nTwo-body interactions: the Coulombic repulsion between individual electrons\n\\[g(r_k,r_l) = \\frac{1}{\\| r_k - r_l \\|}\\]\n\nA constant term: Coulombic repulsion between the nuclear charges\n\\[V_\\text{nn} = \\sum_{A&lt;B} \\frac{N_A N_B}{\\|R_A - R_b\\|} \\]\nwhich is just a constant for a given molecular structure.\n\nWith this, we have fully defined it, and the lowest eigenvalue of this differential operator gives the corresponding ground state energy. To ensure that the ground state describes a multi-electron system, we must impose restrictions on the wavefunction - Fermions have a spin and anti-symmetric permutation symmetry.\n\n\nAs we are describing electrons, they need to obey fermionic antisymmetry. In first quantization (or real-space formulation), the Hamiltonian does not have that property included, so without restrictions on the wavefunctions, it only describes \\(N_\\text{e}\\) negatively charged point-particles without any permutation symmetries or spin.\nWe can add spin conveniently by defining a single-electron wavefunction as a so called spin-orbital: A three-dimensional function \\(\\psi(r) \\in \\mathcal{L}^2(\\mathbb{R}^3)\\), describing the electron in spatial space, augmented with a spin-state \\[\\lvert\\sigma\\rangle \\in \\text{Span}\\left\\{\\lvert \\uparrow \\rangle, \\lvert \\downarrow \\rangle\\right\\} \\equiv \\mathbb{C}^2.\\] If the set of spatial orbitals \\(\\psi_k\\) forms a complete basis in \\(\\mathcal{L}^2(\\mathbb{R}^3)\\), we have an exact description of the electron. A convenient notation is to express the spin component as a function of a spin-coordinate \\(s\\left\\{-1,1\\right\\}\\) and combine spin coordinate \\(s\\) and spatial coordinate \\(r\\) to \\(x=(r,s)\\). In a given set of \\(M\\) spatial orbital, an arbitrary one electron wavefunction can then be expressed as \\[\n\\Psi(x) = \\sum_k^{2M} c_k \\phi(x) = \\sum_{l}^{M} \\psi_{l}(r)\\otimes \\left( c_{2l}\\lvert \\uparrow \\rangle + c_{2l+1} \\lvert \\downarrow \\rangle \\right)\n\\] with the spin-orbitals defined as \\[\n\\phi_k = \\psi_{\\lfloor{i/2}\\rfloor} \\otimes \\lvert \\sigma(k) \\rangle,\\; \\sigma_k=\\begin{cases} \\lvert \\uparrow \\rangle,\\; k \\text{ is even}  \\\\  \\lvert \\uparrow \\rangle,\\; k \\text{ is odd}  \\end{cases}.  \n\\] Many electron functions can then be constructed as linear combinations of anti-symmetric products of single electron functions (so called Slater-Determinants). \\[\n\\Psi\\left(x_1, \\dots, x_{N_\\text{e}}\\right) = \\sum_{m} d_m \\det\\left(\\phi_{m_1},\\dots, \\phi_{m_{N_\\text{e}}}\\right) \\\n\\]"
  },
  {
    "objectID": "Researches/electronic-hamiltonians/index.html#second-quantization",
    "href": "Researches/electronic-hamiltonians/index.html#second-quantization",
    "title": "Electronic Hamiltonians",
    "section": "",
    "text": "Using second quantized language, we can significantly simplify the treatment of electron spin and anti-symmetry with the help of abstract field operators \\(\\hat{\\phi}^\\dagger(x)\\) (\\(\\hat{\\phi}(x)\\)) that create (or annihilate) electrons at spin-position \\(x\\). With them, we can formaly write the electronic Hamiltonian as \\[H = \\int \\hat{\\phi}^\\dagger(x) h(x) \\hat{\\phi}(x) \\operatorname{x}x + \\int \\int \\hat{\\phi}^\\dagger(x)\\hat{\\phi}^\\dagger(y) g(x,y) \\hat{\\phi}(x)\\hat{\\phi}(y) \\operatorname{d}x\\operatorname{d}y+ V_\\text{nn}.\\] The wavefunction is now only required to carry information about electron occupancies at all points in space. When acting on the wavefunction, the one-body part of the operator is first annihilating an electron at point \\(x\\). If no electron is present, this will lead to an energy contribution of zero and otherwise invoke \\(h(x)\\), followed by the restoration of the electron by the creation operator. The two-body part acts in the same way and the integrals ensure that we probe all points in space. In practice, this is unfeasible, and it is convenient to introduce a finite basis in the form of spin orbitals to expand the field operators as \\[\\hat{\\phi}(x) = \\sum_k \\phi(x)_k a_k, \\] leading to the more prominent form of the second quantized Hamiltonian \\[\nH = \\sum_{ij} \\tilde{h}^i{j}a^\\dagger_i a_j \\sum_{ijkl} \\tilde{g}^{ij}_{kl} a^\\dagger_i a^\\dagger_j a_l a_k + V_\\text{nn}.\n\\] The new discretized operators \\(a^\\dagger_k\\) (\\(a_k\\)) are now creating (annihilating) an electron in the spin orbital \\(\\phi_k\\) and the tensors \\(\\tilde{h}^{i}_{j}\\) and \\(\\tilde{g}^{ij}_{kl}\\) are integrals over the one- and two-body operators and the spin orbitals. The one-body integrals are then given by \\[\n\\tilde{h}^{i}_{j} = h^{\\lfloor{i/2}\\rfloor}_{\\lfloor{j/2}\\rfloor} \\langle \\sigma_i \\vert \\sigma_j  \\rangle\n\\] with the spatial part \\[ h^{k}_{l} =  \\langle k \\rvert h \\lvert l \\rangle \\equiv \\int \\psi_k^*(r) h(r) \\psi_l(r) \\operatorname{d}r. \\] In the same way, the spatial part of the two-body integrals is given by \\[ g^{ij}_{kl} = \\langle i j \\rvert g_{12}  \\lvert k l \\rangle \\equiv \\int \\int \\psi_i^*(r_1) \\psi_j^*(r_2) \\frac{1}{|r_1-r_2|} \\psi_k(r_1) \\psi_l(r_2) \\operatorname{d}r_1 \\operatorname{d}r_2.  \\] Note that there exist three different short notations \\[ \\langle ij\\vert kl \\rangle \\equiv \\left(ik \\vert jl \\right) \\equiv \\left[ ij  \\vert lk \\right]  \\] usually referred to as Dirac (physicist, 1212), Mulliken (chemist, 1122) and openfermion (google, 1221) notations. Depending on the convention used, the meaning of the indices \\(g^{ij}_{kl}\\) changes. This is a bit inconvenient, but we can’t change it anymore. Most quantum chemistry packages (pyscf, psi4) use the Mulliken convention, some quantum computing packages adopted the google convention, and the Dirac convention is often found in articles. In the tequila package we tried to automatize most of it away for user convenience. Here is a small example on how to get the integrals using tequila (with either pyscf or psi4 in the back):\n\nimport tequila as tq\n\ngeometry = \"Be 0.0 0.0 0.0\"\nmol = tq.Molecule(geometry=geometry, basis_set=\"6-31G\")\n\nc,h,g = mol.get_integrals(two_body_oderings=\"mulliken\")\n\nfor most applications, the integrals are however processed automatically in the back."
  },
  {
    "objectID": "Researches/electronic-hamiltonians/index.html#example-and-comparison",
    "href": "Researches/electronic-hamiltonians/index.html#example-and-comparison",
    "title": "Electronic Hamiltonians",
    "section": "",
    "text": "Let’s look at first and second quantized Hamiltonians and wavefunctions in an explicit example: The Helium atom (charge \\(N_\\text{He}=2\\) and nuclear coordinate \\(R_\\text{He} = (0,0,0)\\)) in a basis of two spatial orbitals \\(\\left\\{\\psi_0, \\psi_1 \\right\\}\\).\n\n\nFor the neutral electron with 2 electrons, the Hamiltonian is:\n\\[\nH(r_1,r_2) = -\\frac{\\nabla^2}{2} - \\frac{\\nabla^2}{2} - \\frac{2}{\\|r_1\\|} - \\frac{2}{\\|r_2\\|} + \\frac{1}{\\|r_1 - r_2 \\|}  \n\\] and a general two-electron Slater-determinant is written as\n\\[\n\\det\\left(\\phi_k,\\phi_l\\right) = \\frac{1}{\\sqrt{2}} \\left( \\phi_k(r_1) \\phi_l(r_2) - \\phi_l(r_1) \\phi_k(r_2) \\right).\n\\]\nAll possible Slater-determinants in the given basis:\n\nclosed-shell singlets (both electrons in the same spatial orbital):\n\\[\n\\det\\left(\\phi_0 \\phi_1 \\right) = \\frac{1}{\\sqrt{2}}\\psi_0(r_1) \\psi_0(r_1) \\otimes \\left( \\lvert \\downarrow \\uparrow \\rangle - \\lvert \\uparrow \\downarrow \\rangle \\right)\n\\] \\[\n\\det\\left(\\phi_3 \\phi_4\\right) = \\frac{1}{\\sqrt{2}}\\psi_1(r_1) \\psi_1(r_1) \\otimes \\left( \\lvert \\downarrow \\uparrow \\rangle - \\lvert \\uparrow \\downarrow \\rangle \\right)\n\\]\nopen-shell polarized triplets:\n\\[\n\\det\\left(\\phi_1 \\phi_3\\right) = \\left(\\psi_0(r_1) \\psi_1(r_2) + \\psi_1(r_1) \\psi_0(r_2)\\right) \\otimes \\left( \\lvert \\uparrow \\uparrow \\rangle  \\right)\n\\]\n\n\\[\n\\det\\left(\\phi_2 \\phi_4\\right) = \\left(\\psi_0(r_1) \\psi_1(r_2) + \\psi_1(r_1) \\psi_0(r_2)\\right) \\otimes \\left( \\lvert \\downarrow \\downarrow \\rangle  \\right)\n\\]\n\nopen-shell (with broken spin symmetry - recombinable into non-polarized triplet state and one additional singlet state):\n\\[\n\\det\\left(\\phi_0\\phi_2\\right) = \\frac{1}{\\sqrt{2}}\\left(\\phi_0(x_1) \\phi_2(x_2) - \\phi_2(x_1) \\phi_0(x2) \\right)\n\\] \\[\n\\det\\left(\\phi_1\\phi_3\\right) = \\frac{1}{\\sqrt{2}}\\left(\\phi_1(x_1) \\phi_3(x_2) - \\phi_3(x_1) \\phi_1(x2) \\right)\n\\]\n\nA general two-electron wavefunction can then be written as a linear combination of those 6 Determinants (note however, that the different spin-symmetries, i.e. the triplet and the two singlets, usually don’t mix) \\[\n\\Psi(x_1,x_2) = \\sum_{i&lt;j\\in\\left\\{0,1\\right\\}} d_{ij} \\det\\left(\\phi_i\\phi_j\\right)\n\\] Note how this always denotes a two-electron wavefunction. The electron number directly enters the definition of the first quantized Hamiltonian and therefore defines the space onto which the Hamiltonian acts.\nFor numerical procedures it is often necessary to directly express the Hamiltonian in the given basis. Here this would mean to compute all matrix elements \\[\nH_{ij} = \\langle \\det\\left(\\phi_m\\phi_n\\right)\\rvert H \\lvert \\det\\left(\\phi_k\\phi_l\\right) \\rangle.\n\\] In this two electron example this is no problem, the task of computing all matrix elements will however become unfeasible with growing electron number due to the growth of possible determinants. Explicit computation of the Hamiltonian in the given basis is usually only performed within further approximations - e.g. truncated configuration interaction methods that only include slater determinants that differ in a specific number of orbitals from a given reference determinant.\n\n\n\nIn second quantization it is sufficient to compute the one- and two-body integrals given above to define the Hamiltonian. They grow with the fourth power of the basis size rendering the task always feasible.\nA general wavefunction can be constructed from all possible linear combinations of electronic configurations in the given spin-orbital basis - denoted by so called occupation vectors representing which spin orbitals are occupied and not. In this case we have 4 spin orbitals then therefore \\(2^4=16\\) different configurations:\n\nvacuum\n\\[\\lvert 0000 \\rangle\\]\nsingle electron\n\\[\n\\lvert 1000 \\rangle, \\vert 0100 \\rangle, \\lvert 0010 \\rangle, \\lvert 0001\\rangle,\n\\]\ntwo electrons\n\\[\n\\lvert 1100 \\rangle, \\lvert 0011 \\rangle, \\lvert 1001 \\rangle, \\lvert 0110 \\rangle, \\lvert 1010\\rangle, \\lvert 0101 \\rangle,\n\\]\nthree electrons\n\\[\n\\lvert 1110 \\rangle, \\lvert 1101 \\rangle, \\lvert 1011 \\rangle, \\lvert 0111 \\rangle,\n\\]\nfour-electrons\n\\[\n\\lvert 1111 \\rangle.\n\\]\n\nA general wavefunction is then given by\n\\[\\lvert \\Psi \\rangle = \\sum_{k=0}^{16} c_k \\lvert \\text{binary}(k) \\rangle.\\]"
  },
  {
    "objectID": "Researches/molcirc/index.html",
    "href": "Researches/molcirc/index.html",
    "title": "Molecular Quantum Circuits",
    "section": "",
    "text": "Physical insights into the construction of quantum circuits.\nChemical graphs (or Lewis structures/resonance structures) are a powerful abstraction concept, that allows the qualitative prediction of molecular properties or reactions from a remarkably simple structure. Applied to the design of quantum circuits for electronic ground states (the ground states of molecules), chemical graphs allow physical insight into construction, optimization, and interpretation of quantum circuits.\nA detailed description of this methodology is given in arxiv:2207.12421. In this blog entry we will try to approach this circuit design principle through an explicit example."
  },
  {
    "objectID": "Researches/molcirc/index.html#basic-building-blocks",
    "href": "Researches/molcirc/index.html#basic-building-blocks",
    "title": "Molecular Quantum Circuits",
    "section": "Basic Building Blocks",
    "text": "Basic Building Blocks\nIn conventional methodologies of (unitary) coupled-cluster, the wavefunction is generated by exciting electrons from an initial basis state containing a specific number of electrons. A significant distinction between traditional coupled cluster (both unitary and non-unitary) and recent advancements in quantum circuit design lies in the exclusion of higher-order excitations. Instead, the focus is on utilizing a limited set of unitary operations, which are subsequently iteratively applied in a layer-by-layer manner.\nWithin conventional coupled-cluster approaches, a particular type of excitation, such as a single-electron excitation between two orbitals, is typically accounted for only once. To enhance accuracy, higher-order excitations are introduced.\nHowever, in the context of quantum circuit designs, like the one being discussed, this specific excitation might occur multiple times in various sections of the circuit, while the inclusion of higher-order excitations is often bypassed or minimized.\nA simple choice of two basic building blocks for electronic quantum circuits are:\n\nOrbital Rotations (single electron excitations)\n\nPair Excitations (restricted double excitations)\n\nthese operations are for example used in the prominent k-UpCCGSD approach or the separable pair approximation.\nTake for example two orbitals \\(\\phi_0\\) and \\(\\phi_1\\) encoded into four qubits (\\(\\phi_0^\\uparrow,\\phi_0^\\downarrow,\\phi_1^\\uparrow,\\phi_1^\\downarrow\\)) indicating the occupation of the corresponding spin-orbital. The qubit state holding a spin-paired electron pair in orbital \\(\\phi_0\\) is then \\[\n\\lvert 1100 \\rangle.\n\\] If we excite this spin-paired electron into the second spatial orbital we end up with \\[\n\\lvert 1100 \\rangle \\rightarrow \\lvert 0011 \\rangle.\n\\]\nWe can treat the spin-paired electrons bound to the same orbital as a quasi-particle – so-called Hard-core Bosons. A wavefunction constructed from Hard-core Boson states entirely is not invariant to rotations in the underlying orbital basis, as we are confining the spin-paired electrons to occupy a specific choice of orbitals. Graphically we can depict this as\n\n\n\nwhere we have shown the situation in a hydrogen molecule in a minimal basis – one atomic orbital on each hydrogen atom. In this case, the rotation into a different orbital basis clearly leads to a different wavefunction.\nThe two bases depicted in the figure above are an atomic basis (native orbitals), which we will denote as \\(\\phi_L\\) and \\(\\phi_R\\) for left and right, and a molcular basis \\[\n\\phi_\\pm = \\frac{1}{\\sqrt{2}} \\phi_L \\pm \\phi_R.\n\\] The state in the molecular basis as depicted above, can now be writte as \\[\n\\lvert \\Psi \\rangle = c_0 a^\\dagger_{+_{\\downarrow}} a^\\dagger_{+_{\\uparrow}} \\lvert \\rangle + c_1 a^\\dagger_{-_{\\downarrow}}a^\\dagger_{-_{\\uparrow}} \\lvert \\rangle\n\\] using second quantized language. This is a Hardcore-Boson state with one quasi-particle in two possible orbitals. Expressed in the atomic basis, this state looks like \\[\n\\lvert \\Psi \\rangle = \\frac{1}{2} \\left(c_0+c_1\\right) \\left(a^\\dagger_{L_{\\downarrow}}a^\\dagger_{L_{\\uparrow}} + a^\\dagger_{L_{\\downarrow}}a^\\dagger_{L_{\\uparrow}} \\right)\\lvert \\rangle + \\frac{1}{2}\\left(c_0-c_1\\right) \\left(a^\\dagger_{L_{\\downarrow}}a^\\dagger_{R_{\\uparrow}} + a^\\dagger_{R_{\\downarrow}}a^\\dagger_{L_{\\uparrow}} \\right)\\lvert \\rangle,\n\\] with an ionic (both electrons either in \\(L\\) or \\(R\\)) and a neutral (one electron in \\(L\\) and one in \\(R\\)) part. Depending on the choice of the coefficients we can isolate both extremes. In the article this is used to demonstrate how a Hard-core Boson model, with electron pairs confined to the same orbital, is still able to describe bond breaking – i.e. by chosing \\(c_1=-c_0\\) in the wavefunction above.\nQuasi-particle models like the Hard-core Boson approach are therefore dependent on the chosen orbital basis. We can optimize the basis in order to find the best choice for a given quasi-particle wavefunction. These optimizations are however sensitive to initial guesses. Here, molecular graphs serve as a guiding heuristic to initialize good initial guesses for the orbital optimizer. In the original article, this is illustrated in detail on some examples. In the next section, you will find the code for the linear H\\(_4\\) molecule.\nWithin quantum circuits, we can rotate the orbital basis with the second building block: the orbital rotations. This allows us to connect several quasi-particle models in sequence by rotating into different bases, correlating the quasi-particles in that basis and finally rotating back to the initial basis.\nThe molecular graph will take the role as a guiding heuristic helping with placing and initializing the orbital rotation gates as well as the quasi-particle correlators within the quantum circuit."
  },
  {
    "objectID": "Researches/molcirc/index.html#example",
    "href": "Researches/molcirc/index.html#example",
    "title": "Molecular Quantum Circuits",
    "section": "Example",
    "text": "Example\nIn the article the linear H4 system has been used to illustrate the construction of quantum circuits from chemical graphs. We will use the H4 example from the paper and provide the code that reproduces it. First we initialize the molecule and represent the Hamiltonian in a minimal basis. Note that we are using (orthonormalized) atomic orbitals as our second quantized basis and not canonical Hartree-Fock orbitals.\n\nimport tequila as tq\nfrom numpy import pi, eye\n\ngeometry = \"\"\"\nH 0.0 0.0 0.0\nH 0.0 0.0 1.5\nH 0.0 0.0 3.0\nH 0.0 0.0 4.5\n\"\"\"\n\nmol = tq.Molecule(geometry=geometry, basis_set=\"sto-3g\")\n# switch to native orbitals\n# in this case: orthonormalized sto-3g orbitals\nmol = mol.use_native_orbitals()\nH = mol.make_hamiltonian()\nexact = mol.compute_energy(\"fci\")\n\nAs initial state we can construct an SPA circuit using the first graph in the figure above. In this first graph, the H\\(_4\\) molecule is interpreted as two H\\(_2\\) molecules. We initializing the graph, by assigning basis orbitals to the vertices. Here we have 4 basis orbitals (atomic s-type orbitals located on the individual atoms), so the assignment is straightforward.\n\n# graph is\n# H--H  H--H\ngraph = [(0,1),(2,3)]\nUSPA = mol.make_ansatz(name=\"SPA\", edges=graph)\nESPA = tq.ExpectationValue(H=H, U=USPA)\nresult = tq.minimize(ESPA, silent=True)\n\nprint(\"SPA/Atomic error: {:+2.5f}\".format(result.energy-exact))\n\nSPA/Atomic error: +1.09885\n\n\nwe see, that the SPA does not perform well – the reason is that we are currently in an atomic orbital basis. In order to rotate the basis, we add orbital rotations to the initial circuit. Here we will use a static angle that will mix the orbitals in a equally weighted fashion (corresponding to Eq.19 in the article, here explicitly represented with unitary circuits). This represents the SPA in a basis that would correspond to the optimized setting in two isolated H2 molecules (i.e. the first graph)\n\nUR0 = tq.QCircuit()\nfor edge in graph:\n    UR0 += mol.UR(edge[0],edge[1],angle=pi/2)\n\nU0 = USPA + UR0\nE0 = tq.ExpectationValue(H=H, U=U0)\nresult = tq.minimize(E0, silent=True)\n\nprint(\"SPA/Molecular error: {:+2.5f}\".format(result.energy-exact))\n\nSPA/Molecular error: +0.04009\n\n\nThe error is now 40 millihartree. We can bring it further down by allowing the orbitals to relax which can be achieved by adding more orbital rotations with variable angles. We initialize the angles to zero (i.e. we are starting from our previous result as guess). We chose the same pattern as in the \\(U_\\text{RR}\\) circuits in the article\n\n\nCode\nURR0 = mol.UR(0,1,angle=\"b\")\nURR0 = mol.UR(2,3,angle=\"b\")\nURR0 = mol.UR(1,2,angle=\"a\")\n\nU0 = USPA + URR0 + UR0 + URR0 \nE0 = tq.ExpectationValue(H=H, U=U0)\nresult = tq.minimize(E0, silent=True, initial_values=result.variables)\n\nprint(\"SPA/Relaxed-Molecular error: {:+2.5f}\".format(result.energy-exact))\n\n\nSPA/Relaxed-Molecular error: +0.03560\n\n\nAlternatively we can optimize the orbitals with respect to the SPA wavefunction (the strategy from the article) and use this as our orbital basis. As the SPA wavefunction is entirelty within the Hardcode-boson quasiparticle approximation, we can perform the optimization within that approximation reducing our simulation time significantly (see code in the article for the equivalent optimization without HCB approximation)\n\n\nCode\nUHCB = mol.make_ansatz(name=\"HCB-SPA\", edges=graph)\nguess = eye(4)\nguess[0] = [1, 1,0, 0]\nguess[0] = [1,-1,0, 0]\nguess[0] = [0, 0,1, 1]\nguess[0] = [0, 0,1,-1]\nopt = tq.chemistry.optimize_orbitals(circuit=UHCB,molecule=mol, use_hcb=True, initial_guess=guess.T, silent=True)\n\n# update our Hamiltonian\nmol = opt.molecule\nH = mol.make_hamiltonian()\n\nUSPA = UHCB + mol.hcb_to_me()\nE = tq.ExpectationValue(H=H, U=USPA)\nresult = tq.minimize(E, silent=True)\n\nprint(\"SPA/opt-orbitals error: {:+2.5f}\".format(result.energy-exact))\n\n\n/usr/share/miniconda/envs/blogqa/lib/python3.12/site-packages/pyscf/gto/mole.py:1294: UserWarning: Function mol.dumps drops attribute energy_nuc because it is not JSON-serializable\n  warnings.warn(msg)\n\n\nSPA/opt-orbitals error: +0.01626\n\n\nNote, that we can get the same result by providing enough orbital rotations to our circuit.\nTo improve on the SPA model in the optimal basis we take the second graph and apply the following pattern \\[\nU_G = U_R^\\dagger U_C U_R\n\\] that corresponds to 1. rotate into a new orbital basis that resembles the graph structure 2. correlate the quasi particles in this orbital basis 3. rotate back\nIn the following we will do this for the H4 system using the two graphs indicated in the picture above. The first graph is used for the SPA initalization, and the second graph will add further correlation to the initial SPA state. Note that we included an approximation by neglecting one edge in the second graph which we will not correlated. The code-block is identical as the on in the appendix of the article.\n\n\nCode\n# dependencies: tequila &gt;= 1.8.7, pyscf~=1.7, scipy~=1.7\n# suggested quantum backend for optimal performance: qulacs &gt;= 0.3\nimport tequila as tq\nfrom numpy import eye, pi\n\n# Create the molecule object\n# use orthonormalized atomic orbitals as basis\ngeometry = \"h 0.0 0.0 0.0\\nh 0.0 0.0 1.5\\nh 0.0 0.0 3.0\\nh 0.0 0.0 4.5\"\nmol = tq.Molecule(geometry=geometry, basis_set=\"sto-3g\")\nenergies = {\"FCI\":mol.compute_energy(\"fci\")}\n# switch from canonical HF orbitals to orthonormalized STO-3G orbitals\n# to follow notation in the article\nmol = mol.use_native_orbitals()\n\n# Create the SPA circuit for\n# Graph: H -- H    H -- H\n# edges get tuples of orbital-indices assigned\nUSPA = mol.make_ansatz(name=\"SPA\", edges=[(0,1),(2,3)])\n\n# initial guess for the orbitals\n# according to graph in Eq.(17) and orbitals in Eq.(19)\nguess = eye(4)\nguess[0] = [1.0,1.0,0.0,0.0]\nguess[1] = [1.0,-1.,0.0,0.0]\nguess[2] = [0.0,0.0,1.0,1.0]\nguess[3] = [0.0,0.0,1.0,-1.]\n\n# optimize orbitals and circuit parameter\n# PySCF interface\nopt = tq.chemistry.optimize_orbitals(mol, circuit=USPA, initial_guess=guess.T)\nprint(\"Optimized Orbital Coefficients\")\nprint(opt.molecule.integral_manager.orbital_coefficients)\nenergies[\"SPA\"] = opt.energy\n\n# get Hamiltonian with optimized orbitals\nH = opt.molecule.make_hamiltonian()\n\n# initialize rotations to graph in Eq.(21)\n# H    H -- H    H\n# as illustrated in Eq.(24)\n# UR as in Eq.(7) uses spatial orbital indices\nR0 = tq.Variable(\"R0\")\nR1 = tq.Variable(\"R1\")\nUR0 = mol.UR(0,1,angle=(R0+0.5)*pi)\nUR0+= mol.UR(2,3,angle=(R0+0.5)*pi)\nUR1 = mol.UR(1,2,angle=(R1+0.5)*pi)\n\n# initialize correlator according to Eq.(22)\nUC = mol.UC(1,2,angle=\"C\")\n\n# construct the circuit for both graphs\nU = USPA + UR0 + UR1 + UC + UR1.dagger() + UR0.dagger()\n# optimize the energy\nE = tq.ExpectationValue(H=H, U=U)\nresult = tq.minimize(E, silent=True)\nenergies[\"SPA+\"] = result.energy\nprint(energies)\n\n\n\n\nCode\nfor k,v in energies.items():\n    if \"fci\" in k.lower(): continue\n    print(\"{:5} error : {:+2.5f}\".format(k,v-exact))\n\n\nSPA   error : +0.01626\nSPA+  error : +0.00844"
  },
  {
    "objectID": "Researches/molcirc/index.html#further-reading",
    "href": "Researches/molcirc/index.html#further-reading",
    "title": "Molecular Quantum Circuits",
    "section": "Further Reading",
    "text": "Further Reading\n\nOriginal Article\nDivide and Conquer Approach\nSeparable Pair Approximation"
  },
  {
    "objectID": "Researches/molcirc/index.html#dependencies-and-installation",
    "href": "Researches/molcirc/index.html#dependencies-and-installation",
    "title": "Molecular Quantum Circuits",
    "section": "Dependencies and Installation",
    "text": "Dependencies and Installation\nIn order to execute code from this blog entry you need the following dependencies in your environment\npip install tequila-basic\npip install pyscf\n# optional (significantly faster)\npip install qulacs"
  },
  {
    "objectID": "Researches/quantum_classifier/quantum_classifier.html",
    "href": "Researches/quantum_classifier/quantum_classifier.html",
    "title": "Building a Variational Quantum Classifier",
    "section": "",
    "text": "Currently, Convolutional and Deep Neural Networks are at the forefront of research in Machine Learning and Computer Science in general. These networks show the best results in solving complex tasks such as Language Processing and Image Generation. On the other hand, in the area of quantum algorithms, scientists are trying to find algorithms and use cases, where quantum computing creates a unique advantage over classical computers. With the current boom of Machine Learning and Neural Networks, one avenue where quantum advantage might be found would be quantum machine learning. In this area, certain aspects of the classical approach are replaced by quantum systems or data. With the currently available quantum computers, one of the most successful approaches uses Variational Quantum Algorithms, as these work fairly well with only a limited amount of qubits and noise. As a result, there has been a lot of work regarding Quantum Machine Learning using Variational Quantum Algorithms. This tutorial will implement one such algorithm. The implementation will solve a classification task and will do so based on the paper by Pérez-Salinas et al. (2020), where the quantum circuit only uses a single qubit and rotational gates to solve the given task. The simulation will be implemented with the Tequila API (Kottmann et al. 2021) using the qulacs backend for computations. The data and result will be visualized with Matplotlib (Hunter 2007) and Qutip (Johansson, Nation, and Nori 2013).\n\n\nCode\nimport tequila as tq\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport qutip"
  },
  {
    "objectID": "Researches/quantum_classifier/quantum_classifier.html#machine-learning",
    "href": "Researches/quantum_classifier/quantum_classifier.html#machine-learning",
    "title": "Building a Variational Quantum Classifier",
    "section": "Machine Learning",
    "text": "Machine Learning\nIn the most general sense, supervised machine learning aims to train a model using known data to infer knowledge about unknown data based on the training result. In mathematical terms, we are trying to learn some function \\(M: \\mathcal{X} \\to \\mathcal{Y}\\) where \\(\\mathcal{X}\\) is the set of all possible data points and \\(\\mathcal{Y}\\) is the set of labels. We do this using only a relatively small set of data points \\(\\{x_i\\}\\subset \\mathcal{X}\\) with their labels \\(\\{y_i\\}\\subset \\mathcal{Y}\\) and use these to train the parameters \\(\\theta\\) of a model \\(M_\\theta(x)\\). Through the training we get some optimal parameters \\(\\theta_{opt}\\) such that \\(M_{\\theta_{opt}}(x_i)\\) is the best approximation of \\(y_i\\). We can then take an unseen data point \\(x\\in\\mathcal{X}\\) and use our trained model to assign a label \\({y}\\). We do this by calculating \\(M_{\\theta_{opt}}(x)\\) and assigning the result as the label for \\(x\\). One of the most famous examples for this would be the classification of images using a neural network. In that case, the function we are trying to learn maps images to labels. We take a neural network \\(N_{w,\\theta}(x)\\) with some tunable weights \\(w\\) and biases \\(\\theta\\) and train it using some labeled images. After the training, the neural network should have learned some pattern in the data, and we now can insert a new image, which will return the optimal label based on the training images.\nThere are many choices which determine the quality of the result. These consist of:\n\nThe model and its associated parameters\nThe choice of the loss function\nTh optimization routine\n\nThe optimal answers to these questions are in most cases problem specific and chosen based on additional knowledge. The aforementioned choices will be further explained when we need to implement them in section 3."
  },
  {
    "objectID": "Researches/quantum_classifier/quantum_classifier.html#variational-quantum-algorithms-and-quantum-machine-learning",
    "href": "Researches/quantum_classifier/quantum_classifier.html#variational-quantum-algorithms-and-quantum-machine-learning",
    "title": "Building a Variational Quantum Classifier",
    "section": "Variational Quantum Algorithms and Quantum Machine Learning",
    "text": "Variational Quantum Algorithms and Quantum Machine Learning\nWe can view Quantum Machine Learning from two different perspectives. From a machine learning point of view, we simply use a parameterized quantum circuit as our model, and try to adapt all other parts to fit the concepts of this circuit. The other viewpoint considers general Variational Quantum Algorithms (VQAs), which are hybrid algorithms that involve a parameterized quantum circuit and classical computation to optimize the parameters. These VQAs can also be used to solve tasks that are native to machine learning, which would lead us again to quantum machine learning.\nWe can examine the basic structure of a VQA through a simple example using Tequila:\n\n1a = tq.Variable(name=\"a\")\nb = tq.Variable(name=\"b\")\n2Unitary = tq.gates.Rx(np.pi * a, target=0) + tq.gates.Ry(np.pi * b, target=0)\n\n3Observable = tq.paulis.X(0) + tq.paulis.Y(0)\n4Objective = tq.ExpectationValue(U=Unitary, H=Observable)\n\n5initial_values = {a: 1, b:1}\n6minimized = tq.minimize(Objective,variables=[a,b],initial_values=initial_values, silent=True)\n7print('minimal value of the objective: ', minimized.energy)\nprint('optimal variable values: \\n', minimized.angles)\n\n\n1\n\nInitialize tunable variables, that will be optimized later\n\n2\n\nCreate a unitary/quantum circuit using a given problem structure\n\n3\n\nDefine an Observable whose expectation we want to measure\n\n4\n\nDefine the Objective/Loss function we want to minimize as the expectation value of the Observable after applying the Unitary\n\n5\n\nDefine some initial values for the variables we want to optimize\n\n6\n\nMinimize the Objective through one of many different minimization methods to compute optimal variable assignments\n\n7\n\nReturn the calculated parameters of the circuit and the minimal value of the Objective\n\n\n\n\nminimal value of the objective:  -1.4142135607453392\noptimal variable values: \n a : 0.2499994506269027\nb : -0.499978415813107\n\n\n\nThe bulk of the work is done in the minimization step, where we iteratively calculate the current value of the objective and its gradient and use these values to determine some small changes to the parameters so that we can further improve the result. The objective describes the error of our circuit and the minimum would be the parameter assignment that has the least error. Using this setup, we can now build a VQA to do classification."
  },
  {
    "objectID": "Researches/quantum_classifier/quantum_classifier.html#the-data-for-classification",
    "href": "Researches/quantum_classifier/quantum_classifier.html#the-data-for-classification",
    "title": "Building a Variational Quantum Classifier",
    "section": "3.1 The data for Classification",
    "text": "3.1 The data for Classification\nWe first need to determine the data we want to classify. For general data, there are several complicated questions that arise when trying to insert a data point in the circuit. The usual data, like numbers, vectors, matrices or higher dimensional data are best represented using a classical computer, and we need to figure out how to transform the representation as many bits onto a quantum circuit with only a few qubits. We will address this in section 3.3. In order to visualize the data and to assess the quality of our training, we will be using elements of \\(\\mathbb{R}^2\\). We further restrict ourselves to the area \\([0,2]\\times [0,2]\\). In this area we draw three partially overlapping circles, resulting in four total classes including the background (Figure 1). We create three different lists. visual to simply show what the classes look like, test_data to see if the trained circuit is able to classify correctly, and training_data with associated labels, which is used to train the model.\n\n\nCode\ndef get_random_point():\n    return random.uniform(0.0, 2.0), random.uniform(0.0, 2.0)\n\n\n\ndef calculate_label(datapoint):\n    circle1 = ((0.0,0.0), np.pi / 2)\n    circle2 = ((0.5,1.75), np.pi / 8)\n    circle3 = ((2.0,2.0), np.pi / 4)\n    if np.linalg.norm((datapoint[0] - circle1[0][0], datapoint[1]- circle1[0][1])) &lt; circle1[1]:\n        return 1\n    elif np.linalg.norm((datapoint[0] - circle2[0][0], datapoint[1]- circle2[0][1])) &lt; circle2[1]:\n        return 2\n    elif np.linalg.norm((datapoint[0] - circle3[0][0], datapoint[1]- circle3[0][1])) &lt; circle3[1]:\n        return 3\n    else:\n        return 0\n    pass\n\n\ndef visualize_points(data_set):\n    colors = []\n    for data_point in data_set:\n        if data_point[2] == 1:\n            colors.append('r')\n        elif data_point[2] == 2:\n            colors.append('g')\n        elif data_point[2] == 3:\n            colors.append('y')\n        else:\n            colors.append('b')\n    fig, ax = plt.subplots(figsize=(5, 5))\n    ax.scatter([dp[0] for dp in data_set], [dp[1] for dp in data_set], c=colors, marker='.')\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    pass\n\n\n\nvisual = [get_random_point() for _ in range(10000)]\nvisual = [(dp[0],dp[1],calculate_label(dp)) for dp in visual]\n\ntest_data =[get_random_point() for _ in range(1000)]\n\ntraining_data = [get_random_point() for _ in range(250)]\nlabels = [calculate_label(dp) for dp in training_data]\n\nvisualize_points(visual)\n\n\n\n\nFigure 1: Data to be classified"
  },
  {
    "objectID": "Researches/quantum_classifier/quantum_classifier.html#measurement-and-interpretation",
    "href": "Researches/quantum_classifier/quantum_classifier.html#measurement-and-interpretation",
    "title": "Building a Variational Quantum Classifier",
    "section": "3.2 Measurement and interpretation",
    "text": "3.2 Measurement and interpretation\nBefore looking at our Ansatz, we will first look at what we do with the final state. When we insert a data point in the circuit, we will find some final state, which we now need to measure, to infer the classification result. The circuit prepares a state \\(|\\phi\\rangle = c_0 |0\\rangle + c_1 |1\\rangle\\). If we simply measure in the computational basis, we have the result \\(|0\\rangle\\) with a probability of \\(p_0=|c_0|^2\\) and the result \\(|1\\rangle\\) with probability \\(p_1=|c_1|^2\\).\nFor more information, we need to prepare the final state several times in order to measure the expectation value of some observable. The simplest case would be to measure the observable \\(\\Lambda = |1\\rangle\\langle 1| = \\begin{pmatrix} 0 & 0\\\\ 0 & 1 \\end{pmatrix}\\), where we would receive \\[\\langle \\Lambda \\rangle = \\langle \\phi |\\Lambda| \\phi \\rangle = \\langle \\phi | 1\\rangle\\langle 1|\\phi \\rangle =  c_1^* c_1 = |c_1|^2 =  p_1\\] which lets us classify based on the probabilities. Binary data with only two classes \\(0\\) and \\(1\\) can be labeled in a straightforward manner: If \\(p_1 &lt; 0.5\\) we choose class \\(0\\), otherwise we assign the label \\(1\\). For our more general case with four classes, we have several different ways we could assign labels:\n\nSeveral thresholds\nWe can directly generalize the way we classify by adding more divisions of \\(p_1\\). \\(p_1\\) takes values in \\([0,1]\\). If we divide this interval into \\(n\\) different sections, we have a way of distinguishing \\(n\\) classes. We can interpret this on the Bloch sphere as making parallel slices of the sphere so that we get \\(n\\) bands. We than classify based on the band the state is on.\n\n\nAssignment using Projectors\nOne interpretation of the binary classification, would be to think of \\(p_1\\) as the fidelity between \\(|1\\rangle\\) and \\(|\\phi\\rangle\\). This would mean, that we choose label \\(1\\) if the fidelity between \\(|1\\rangle\\) and \\(|\\phi\\rangle\\) is bigger than between \\(|0\\rangle\\) and \\(|\\phi\\rangle\\). This allows for a different kind of generalization: We determine \\(n\\) different states \\(|\\psi_i\\rangle\\) as the representatives of \\(n\\) different classes. We then calculate the fidelity between the output of the circuit with all of these \\(n\\) label states and assign the label that is closest to the output, i.e. the one with the highest fidelity. We can compute these fidelities by calculating the expectation value of the projector onto the label states \\(\\Lambda_i = |\\psi_i\\rangle\\langle\\psi_i|\\), which would be \\[\n\\langle\\Lambda_i\\rangle = \\langle \\phi |\\psi_i\\rangle\\langle\\psi_i| \\phi \\rangle = \\langle \\psi_i |\\phi\\rangle^*\\langle\\psi_i| \\phi \\rangle = |\\langle\\psi_i| \\phi \\rangle|^2.\n\\] The \\(i\\) for which this value is maximal will be the label for \\(| \\phi \\rangle\\). We will use this method to assign labels using the trained circuit. We still need to examine how we choose these label states. The criterion for these should be, that they are as far apart as possible. One way to specify that would be to find states such that \\(min_{i,j} |\\langle\\psi_i|\\psi_j \\rangle |^2\\) is as big as possible. In terms of the Bloch Sphere we search \\(n\\) points on the sphere that have the highest pairwise distance. For 4,6,8,12 and 20 classes it is fairly intuitive that we get these label states as the vertices of the respective regular polyhedron. For all other amounts this is a harder question to answer. For 4 classes the label states can be seen in (Figure 2).\n\ndef get_label_state(n):\n    if n == 0:\n        return tq.QubitWaveFunction.from_array([1,0])\n    if n == 1:\n        return tq.QubitWaveFunction.from_array([1 / np.sqrt(3), np.sqrt(2/3)])\n    if n == 2:\n        return tq.QubitWaveFunction.from_array([1 / np.sqrt(3),np.exp(1j * 2 * np.pi / 3) * np.sqrt(2 / 3)])\n    if n == 3:\n        return tq.QubitWaveFunction.from_array([1 / np.sqrt(3), np.exp(-1j * 2 * np.pi / 3) * np.sqrt(2 / 3)])\n    pass\n\n\ndef fidelity(circuit, label_state):\n    \"\"\"\n\n    :param circuit: The quantum circuit\n    :param label_state: The wave function of the label state\n    :return: The expectation value of the projector onto func in the state prepared by circuit\n    \"\"\"\n    projector = tq.paulis.Projector(wfn=label_state)\n    objective = tq.ExpectationValue(circuit, projector)\n    return objective\n\n\n\nCode\ndef show_label_states():\n    x1 = qutip.Qobj(tq.QubitWaveFunction.to_array(get_label_state(0)))\n    x2 = qutip.Qobj(tq.QubitWaveFunction.to_array(get_label_state(1)))\n    x3 = qutip.Qobj(tq.QubitWaveFunction.to_array(get_label_state(2)))\n    x4 = qutip.Qobj(tq.QubitWaveFunction.to_array(get_label_state(3)))\n\n    fig = plt.figure(figsize=(13,12), layout='constrained')\n    ax1 = fig.add_subplot(1,3,1, projection='3d')\n    ax1.set_title(\"label states\")\n    ax2 = fig.add_subplot(1,3,2, projection='3d')\n    ax2.set_title(\"Real part\")\n    ax2.azim = 90\n    ax2.elev = 0\n    ax3 = fig.add_subplot(1,3,3, projection='3d')\n    ax3.set_title(\"Top down view\")\n    ax3.elev = 90\n\n    b = qutip.Bloch(fig=fig, axes=ax1)\n    b.vector_width = 2\n    b.add_states([x1,x2,x3,x4])\n    b.render()\n\n    b2 = qutip.Bloch(fig=fig, axes=ax2)\n    b2.vector_width = 2\n    b2.add_states([x1,x2,x3,x4])\n    b2.render()\n\n    b3 = qutip.Bloch(fig=fig, axes=ax3)\n    b3.vector_width = 2\n    b3.add_states([x1,x2,x3,x4])\n    b3.render()\n\nshow_label_states()\n\n\n\n\n\nFigure 2: Label states for 4 classes from several angles\n\n\n\n\n\n\nAssignment using the full state\nIn the previous solution, we needed one measurement of an expectation value for each class. For a circuit with only one qubit, this seems unreasonably high, as the full state is only characterized by two real numbers and given the full state we could easily calculate the fidelities with all label states. So we might be able to get rid of the scaling number of calculated expectation values if we are able to just calculate the full state with a constant number of expectation values. This is indeed possible using a state tomography protocol, which allows us to obtain the full state with only three expectation values for a single qubit (Schmied 2016). We can visualize the information we gain from a single expectation value using the Bloch sphere. For this we look at the state \\(|-i\\rangle\\) and the three pojectors \\(|0\\rangle\\langle 0|\\), \\(|+\\rangle\\langle +|\\),\\(|i\\rangle\\langle i|\\). Assuming we don’t know that the sytem is in state \\(|-i\\rangle\\), we will figure this out using the three expectation values of the projectors. The expectation value of \\(|0\\rangle\\langle 0|\\) will have the result \\(0.5\\), which gives us the information that the state has to be on the ring seen in Figure 3. \\(\\langle (|+\\rangle\\langle +|)\\rangle\\) will also be \\(0.5\\), so that we are now restricted to two possible states, the ones where both rings intersect in Figure 3. After the final measurement we will now be able to fully determine the final state of the system. This works for all one-qubit states.\nTheoretically this an also be done for more qubits, but requires an exponential amount of expectation values, as the dimensionality of the state also grows exponentially.\n\n\nCode\ndef add_line(bloch, x,y):\n    bloch.add_arc(x, y)\n    bloch.add_arc(-x, y)\n    bloch.add_arc(x, -y)\n    bloch.add_arc(-y, -x)\n\ndef tomography():\n    fig = plt.figure(figsize=(10,10), layout='constrained')\n    ax1 = fig.add_subplot(1,3,1, projection='3d')\n    ax2 = fig.add_subplot(1,3,2, projection='3d')\n    ax3 = fig.add_subplot(1,3,3, projection='3d')\n\n    b1 = qutip.Bloch(fig=fig, axes=ax1)\n    b2 = qutip.Bloch(fig=fig, axes=ax2)\n    b3 = qutip.Bloch(fig=fig, axes=ax3)\n    X = np.arange(-1, 1, 0.1)\n    Y = np.arange(-1, 1, 0.1)\n    Z = np.ones((20,20))\n\n    b1.add_vectors([0,0,1])\n    add_line(b1, np.array([1,0,0]), np.array([0,1,0]))\n    ax1.plot_surface(X, Y, Z)\n    b2.add_vectors([1,0,0])\n    add_line(b2, np.array([1,0,0]), np.array([0,1,0]))\n    add_line(b2, np.array([0,1,0]), np.array([0,0,1]))\n    b2.add_points([[0,0],[1,-1],[0,0]])\n\n    b3.add_vectors([0,1,0])\n    b3.add_points([0,-1,0])\n\n    b1.render()\n    b2.render()\n    b3.render()\n    plt.show()\n\ntomography()\n\n\n\n\n\nFigure 3: Possible states after one, two and three measurements. The arrow describes the projector we are measuring"
  },
  {
    "objectID": "Researches/quantum_classifier/quantum_classifier.html#ansatz",
    "href": "Researches/quantum_classifier/quantum_classifier.html#ansatz",
    "title": "Building a Variational Quantum Classifier",
    "section": "3.3 Ansatz",
    "text": "3.3 Ansatz\nAfter figuring out how we turn the final state into a classification result, we can look at the circuit that prepares the final state. We need to build a parameterized circuit, that allows us to insert a datapoint \\(x\\) and parameters \\(\\theta\\). We will be limited to gates that operate on one qubit, so our main choices are rotations \\(R_x,R_y,R_z\\). Using only rotations, it would be impossible to separate the data nonlinearly if we inserted the datapoint only once. We need a layered structure, where each layer inserts the datapoint and some of the parameters. The entire circuit has the form \\[\nU(\\theta,x) = L(N)...L(1)\n\\] where \\(N\\) is the amount of layers in our circuit. One layer is then composed of several rotations \\[\nL(i)= R_z(\\theta_{2i+1}+x_2) R_y(\\theta_{2i} + x_1)\n\\] using a set of \\(2N\\) angles \\(\\theta\\) and the datapoint \\(x=(x_1,x_2)\\). The amount of layers determines the depth of our circuit, as we apply two gates per layer. Also, the amount of computation needed increases as we introduce more layers, as we get two parameters for each layer.\nWhy did we choose the setup with one \\(R_z\\) and \\(R_y\\) gate? This choice was to a certain level arbitrary. We chose two gates as we use data from \\(\\mathbb{R}^2\\), where every gate encodes one of the two values. The choice for the type of gates was made, so that it is possible to rotate every state to every other state. Any combination of two different rotations would be satisfactory, so this choice was arbitrary.\nWe could adapt the layers to encode more parameters, by introducing weights \\(w\\) for every insertion of the datapoint. One layer would then look like \\[\nL(i)= R_z(\\theta_{2i+1}+w_{2i+1} x_2) R_y(\\theta_{2i} + w_{2i}x_1).\n\\] In theory this increases the expressiveness of the circuit. If we replaced the unweighted layers with their weighted alternatives, we only need to set all weights to 1 and would receive the same result as the unweighted case. The computation will be more expensive, but the final result should always be as good as for the unweighted case or even better. We will see if this assumption holds true in section 4.\n\ndef create_circuit(x, angles):\n    circuit = tq.QCircuit()\n    for i in range(len(angles) // 2):\n        circuit += tq.gates.Ry(angles[2*i] + x[0],0) + tq.gates.Rz(angles[2*i+1] + x[1],0)\n    return circuit"
  },
  {
    "objectID": "Researches/quantum_classifier/quantum_classifier.html#loss-function",
    "href": "Researches/quantum_classifier/quantum_classifier.html#loss-function",
    "title": "Building a Variational Quantum Classifier",
    "section": "3.4 Loss function",
    "text": "3.4 Loss function\nNow we have everything required to calculate a label for a datapoint and will now examine how we determine the correctness of this assignment. This is done through the loss function. Given a set of values for the parameters in the circuit and the training data, the loss function evaluates how close the result of the circuit is to the actual labels. One possible function is \\[\n\\lambda(X,\\theta) = \\frac{1}{|X|}\\sum_{x\\in X} (1-|\\langle \\psi_c(x)| U(\\theta,x)|0\\rangle|^2)^2\n\\] where \\(| \\psi_c(x)\\rangle\\) is the label state of the class that \\(x\\) belongs to and \\(X\\) is the training data. Each element of the sum is one minus the fidelity between the circuit result and the correct label, therefore each element takes values in \\([0,1]\\). If for one \\(x\\) it is \\(0\\), that means the fidelity between the correct label and the circuit result is \\(1\\), so the circuit has transformed \\(x\\) in the correct state. Alternatively, if the fidelity is low, this datapoint would contribute a fairly big amount to the loss function. In the optimal case, that everything gets classified correctly, the value of the loss function would be zero. In order to get a proper classifier, all that is needed now, is the values of \\(\\theta\\) such that \\(\\lambda\\) is minimal. We try to find these values in the optimization step in the next section.\nFor the optimization it is useful to rewrite \\(\\lambda\\). Instead of looking at fidelities between states, we express the loss in terms of expectation values \\[\n\\lambda(X,\\theta) = \\frac{1}{|X|}\\sum_{x\\in X} (1-\\langle 0|U(\\theta,x)^T \\Psi_c(x) U(\\theta,x)|0\\rangle)^2\n\\] where \\(\\Psi_c(x) = | \\psi_c(x)\\rangle \\langle \\psi_c(x)|\\) is the projector defined by the correct label state of \\(x\\in X\\). This is also the way it is implemented.\n\ndef cost(train_set, labels, parameters, circuit):\n    \"\"\"\n\n    :param train_set: A list of the datapoints in the training set\n    :param labels: A list of the labels of the datapoints\n    :param parameters: The values of the parameters in the parameterized circuit\n    :param circuit: A function that returns a circuit and takes one datapoint and\n                    the parameters as arguments\n    :return: The value of the loss function\n    \"\"\"\n    loss = 0.0\n1    for i in range(len(train_set)):\n        qc = circuit(train_set[i],parameters)\n        target = get_label_state(labels[i])\n2        fid = fidelity(qc, target)\n3        loss = loss + ((1-fid) ** 2)\n    return loss / len(train_set)\n\n\n1\n\nWe iterate through the training data and calculate the impact of that datapoint on the loss function\n\n2\n\nThe fidelity between the circuit result and the correct label state is expressed as a expectation value\n\n3\n\nA value in \\([0,1]\\) gets added based on how good the current datapoint is treated by the circuit"
  },
  {
    "objectID": "Researches/quantum_classifier/quantum_classifier.html#minimization",
    "href": "Researches/quantum_classifier/quantum_classifier.html#minimization",
    "title": "Building a Variational Quantum Classifier",
    "section": "3.5 Minimization",
    "text": "3.5 Minimization\nThe training requires us to find values of the tunable parameters \\(\\theta\\) such that the loss function \\(\\lambda(X,\\theta)\\) is minimal. There are many different methods with different advantages and properties to solve optimization problems. Many of these optimization algorithms have a local approach for minimization, meaning we iteratively calculate values \\(\\theta_0,\\theta_1,...,\\theta_n\\) where we calculate \\(\\theta_i\\) based on some calculation using \\(\\theta_{i-1}\\). For most algorithms, we compute or approximate the gradient \\(\\nabla \\lambda(X,\\theta_{i-1})\\) or the hessian \\(H \\lambda(X,\\theta_{i-1})\\) in order to calculate \\(\\theta_{i}\\). The gradient is the vector of the first derivatives and the hessian is the matrix containing all second derivatives. The calculation of these values is fairly expensive and explains why they are in many cases only approximated.\nWe can view our loss function as a function of expectation values, which are in turn functions of the parameters and a single datapoint. This means for the calculation of the gradient we can split the function using the chain rule: \\[\n\\frac{\\partial \\lambda(X,\\theta)}{\\partial\\theta} = \\sum_{x\\in X}\\frac{\\partial \\lambda(X,\\theta)}{\\partial \\langle E_x\\rangle} \\frac{\\partial \\langle E_x\\rangle}{\\partial\\theta}\n\\] where \\(\\langle E_x\\rangle = \\langle 0|U(\\theta,x)^T \\Psi_c(x) U(\\theta,x)|0\\rangle\\). This way we get two parts that we can examine separately. The left part can be done through automatic differentiation like it is usually done for differentiation problems. The right part concerns the differentiation of a quantum circuit with respect to one of the parameters, for which there are several methods, two of these are numerical differentiation and the parameter shift rule (Crooks 2019). Using the result of these calculations, we now have the gradient to determine the next step in an iteration of the optimization.\nWithout further examining whether our choice theoretically leads to a good result, we will be choosing BFGS as our optimization algorithm as used in (Pérez-Salinas et al. 2020).\n\nlayers = 5\nparam_unweighted = [tq.Variable('p' + str(i)) for i in range(layers * 2)]\ninitials_unweighted = {key: random.uniform(0, 2 * np.pi) for key in param_unweighted}\n\n\n\nObjective1 = cost(training_data, labels, param_unweighted, create_circuit)\n\nresult1 = tq.minimize(objective=Objective1, initial_values=initials_unweighted,  silent=True, backend=\"qulacs\")\nprint(result1.history.plot('energies', label='loss'))\nprint(result1.history.plot('angles', label=\"\"))\n\n\n\n\nNone\nNone"
  },
  {
    "objectID": "Researches/quantum_classifier/quantum_classifier.html#results",
    "href": "Researches/quantum_classifier/quantum_classifier.html#results",
    "title": "Building a Variational Quantum Classifier",
    "section": "3.6 Results",
    "text": "3.6 Results\nWe have done the optimization and have the optimal parameter assignment stored in result1.angles. With these results we can run through our test set and assign the labels according to the method from section 3.2. For this, we calculate the fidelity to the different label states and assign the label with the highest fidelity. We can use the visualization we used earlier for Figure 1 to compare the correct labels with the trained circuit. By computing the correct label through calculate_label(), we can also calculate the percentage of correct guesses. As a reference, guessing randomly would give us a success rate of \\(0.25\\).\n\ndef assign_label(datapoint, opt_params, circuit, param):\n    qc = circuit(datapoint,param)\n    max_fid = 0\n    choice = 0\n    for i in range(4):\n        H = tq.paulis.Projector(get_label_state(i))\n        Exp = tq.ExpectationValue(U=qc, H=H)\n        fid = tq.simulate(Exp, variables=opt_params)\n        if fid &gt; max_fid:\n            max_fid = fid\n            choice = i\n    return choice\n\ndef test_and_visualize(testing_data, opt_params, circuit, param):\n    labeled_set = []\n    num_correct = 0\n    for x in testing_data:\n        label = assign_label(x, opt_params, circuit, param)\n        if label == calculate_label(x):\n            num_correct += 1\n        labeled_set.append((x[0], x[1], label))\n\n    print(\"Success Rate: \", num_correct / len(testing_data))\n    visualize_points(labeled_set)\n\ntest_and_visualize(test_data, result1.angles, create_circuit, param_unweighted)\n\nSuccess Rate:  0.877\n\n\n\n\n\nAs we can see, the circuit has learned the data fairly well, leading to a success rate of \\(88\\%\\) using 5 layers. In comparison to other machine learning algorithms like neural networks, this result holds up well, as we are only using 10 parameters in our circuit, and are still able to perform fairly well. For many other systems the amount of required parameters would rise significantly faster."
  },
  {
    "objectID": "Contribution/Contribution.html",
    "href": "Contribution/Contribution.html",
    "title": "Contribution",
    "section": "",
    "text": "If you find any bugs or inconveniences in tequila please don’t be shy and let us know. You can do so either by raising an issue on github or contact us directly.\n\nHow can you upload your own tutorial or research page?\n\nHow to install Quarto\nFor installing and using Quarto correctly, please take a look at some helpul links:\n\nInstallation: https://quarto.org/docs/get-started/\nQuarto basics: https://quarto.org/docs/get-started/hello/rstudio.html\n\n\n\nHow to set up the environment and compile the blog\n\nClone from source\nCreate a conda environment with the name ‘BlogQa’ (concrete: opt/anaconda3/envs/BlogQA/bin/python3):\n\n\nconda create -n BlogQA python=3.9\nconda env list\npython -m ipykernel install –user –name=BlogQA\nconda install ipykernel\nconda activate BlogQA\n\n\nInstall the dependecies as well as everthing needed regarding Jupyter and the BlogQA Kernel: In particular these are the following ones:\n\n\nconda create -n BlogQA python=3.9\npip install pyyaml\npip install nbformat\npip install nbclient\npip install numpy\npip install scipy\npip install matplotlib\npip install tequila-basic\npip install qulacs\npip install pyscf\n\n\nPreview with “quarto preview”\n\n\n\nHow to add your contribution\nFirst, please make sure that only ipynb-files or quarto-files are allowed as tutorials or research pages for the website. Within these, the following HTML parameters can be placed in a separate markdown at the top, before the actual content begins:\n\ntitle: \"Any titel\"\nauthor: \"Your full name\"\ndate: \"yyyy-mm-dd\"\ncategories: [code]\nimage-width: \"1cm\"\nimage-height: \"1cm\"\nformat:\n    html:\n        code-fold: false\n        eval: true\njupyter: blogqa\n\nBegin and end this section with three hyphens (—)\n\n\nHow to submit\n\nMake a fork of tequila to your own github account.\nCheckout the devel branch and make sure it is up to date with the main github repository.\nCreate and checkout a new branch from devel via git branch pr-my-branch-name followed by git checkout pr-my-branch-name. By typing git branch afterwards you can check which branch is currently checked out on your computer.\nNavigate to the tutorial/ researches folder and create your own subfolder, which you name with the tutorial/research name. This subfolder should include your ipynb/quarto-file (as well as other necessary files, such as png-files for importing images).\nIntroduce changes to the code and commit them with git.\nPush the changes to your github account.\nLog into github and create a pull request to the main github repository. The pull request should be directed to the devel branch (but we can also change that afterwards).\n\nIf you plan to introduce major changes to the base library it can be beneficial to contact us first. This way we might be able to avoid conflicts before they arise.\nIf you used tequila for your research, feel free to include your algorithms here, either by integrating it into the core libraries or by demonstrating it with a notebook in the tutorials section. If you let us know about it, we will also add your research article in the list of research projects that use tequila (see the github repos README)."
  },
  {
    "objectID": "Tutorials/optimizer/Optimizers.html",
    "href": "Tutorials/optimizer/Optimizers.html",
    "title": "Optimizer",
    "section": "",
    "text": "Explore the two different optimizers available through Tequila interfaces:\nA native gradient descent (GD) optimizer, as well as interfaces for SciPy optimizers.\nimport tequila as tq\nimport numpy as np\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "Tutorials/optimizer/Optimizers.html#the-gd-optimizer",
    "href": "Tutorials/optimizer/Optimizers.html#the-gd-optimizer",
    "title": "Optimizer",
    "section": "The GD Optimizer",
    "text": "The GD Optimizer\nWe will start this tutorial by looking at the GD optimizer. Here is an overview over the available optimization methods.\n\n1tq.show_available_optimizers(module=\"gd\")\n\n\n1\n\nGet a list of all available optimization methods for the GD optimizer\n\n\n\n\navailable methods for optimizer module gd\nmethod               | optimizer module\n--------------------------\nadam                 | gd\nadagrad              | gd\nadamax               | gd\nnadam                | gd\nsgd                  | gd\nmomentum             | gd\nnesterov             | gd\nrmsprop              | gd\nrmsprop-nesterov     | gd\nspsa                 | gd\n\n\nA variety of methods are available for optimization. Here, ‘sgd’ refers to the standard gradient descent algorithm, without momentum. Like all Tequila optimizers, the GD optimizer has a minimize function and most of the arguments are the same. However, there is one important difference: the GD optimizer takes a learning rate (lr). This parameter mediates step size in all of the GD optimizer methods; It is a positive float which scales the step in the direction of the gradient.\nWe will now optimize \\(O_1\\), our two-qubit expectation value, starting with angles equivalent to \\(\\frac{1}{4}\\pi\\) for all four variables, and optimizing via the ‘Adam’ method.\n\n1init={'a':0.25,'b':0.25,'c':0.25,'d':0.25}\n2lr=0.1\n\n3adam_result=tq.minimize(objective=O1,\n                        lr=lr,\n                        method='adam',\n                        maxiter=80,\n                        initial_values=init,\n4                        silent=True)\n\n\n1\n\nInitialize all four variables to \\(\\frac{1}{4}\\pi\\)\n\n2\n\nSet the learning rate to 0.1\n\n3\n\nOptimize the objective \\(O_1\\) using the Adam method with learning rate 0.1 and maximal iterations 80\n\n4\n\nSet silent=True to suppress output\n\n\n\n\nThe plots below show the trajectory of both the value of the objective and the values of the angles as a function of time.\n\n1adam_result.history.plot('energies')\n2adam_result.history.plot('angles')\nprint('best energy: ',adam_result.energy)\nprint('optimal angles: ',adam_result.angles)\n\n\n1\n\nPlot energy from Adam optimization\n\n2\n\nPlot angles from Adam optimization\n\n\n\n\n\n\n\n\n\n\nbest energy:  -1.6179942179567814\noptimal angles:  a : 0.14749685873977889\nb : 1.0037045157384548\nc : -0.0020942234687567523\nd : -0.4982090391144128\n\n\n\nWe see that, apart from a few hiccups, all the angles converge to optimimal values.\nLet’s repeat what we did above, but with a few other methods! Here’s RMSprop:\n\n1init={'a':0.25,'b':0.25,'c':0.25,'d':0.25}\n2lr=0.01\n\n3rms_result=tq.minimize(objective=O1,\n                       lr=lr,\n                       method='rmsprop',\n                       maxiter=80,\n                       initial_values=init,\n                       silent=True)\n\nprint('RMSprop optimization results:')\n\n4rms_result.history.plot('energies')\n5rms_result.history.plot('angles')\nprint('best energy: ',rms_result.energy)\nprint('optimal angles: ',rms_result.angles)\n\n\n1\n\nInitialize all four variables to \\(\\frac{1}{4}\\pi\\)\n\n2\n\nSet the learning rate to 0.01\n\n3\n\nOptimize the objective \\(O_1\\) using the RMSprop method with learning rate 0.01 and maximal iterations 80\n\n4\n\nPlot energy from RMSprop optimization\n\n5\n\nPlot angles from RMSprop optimization\n\n\n\n\nRMSprop optimization results:\nbest energy:  -1.6180339887498942\noptimal angles:  a : 0.14758361765043307\nb : 0.9999999875406629\nc : 1.498301743024053e-08\nd : -0.5\n\n\n\n\n\n\n\n\n\n… And now let’s take a look at Momentum:\n\n1init={'a':0.25,'b':0.25,'c':0.25,'d':0.25}\n2lr=0.1\n\n3mom_result=tq.minimize(objective=O1,\n                       lr=lr,\n                       method='momentum',\n                       maxiter=80,\n                       initial_values=init,\n                       silent=True)\n\nprint('momentum optimization results:')                 \n4mom_result.history.plot('energies')\n5mom_result.history.plot('angles')\nprint('best energy: ',mom_result.energy)\nprint('optimal angles: ',mom_result.angles)\n\n\n1\n\nInitialize all four variables to \\(\\frac{1}{4}\\pi\\)\n\n2\n\nSet the learning rate to 0.1\n\n3\n\nOptimize the objective \\(O_1\\) using the Momentum method with learning rate 0.1 and maximal iterations 80\n\n4\n\nPlot energy from Momentum optimization\n\n5\n\nPlot angles from Momentum optimization\n\n\n\n\nmomentum optimization results:\nbest energy:  -1.617816413662847\noptimal angles:  a : 0.15313030387151333\nb : 0.9975386772455798\nc : -0.0008949810647826415\nd : -0.5018117058023004\n\n\n\n\n\n\n\n\n\nNote that when using the RMSprop method, we reduced the learning rate from \\(0.1\\) to \\(0.01\\). Different methods may be more or less sensitive to choices of initial learning rate. Feel free to revisit previous examples, and experiment with different learning rates, or initial parameters to gain insight into the sensitivity of different methods.\nAccelerated gradient descent to achieve numerical accuracy:\nOne issue with gradient descent is that they are slow when it comes to converging to machine precision. As optimization nears an optimum, the gradient goes to zero, which means that each successive step is smaller and smaller, and convergence slows down. This is not really a problem in machine learning, where convergence to many digits is not needed, but it is an issue in chemistry.\nThe standard method used to converge quantum chemical calculations is an accelerated form of gradient descent alternatively called Direct Inversion of the Iterative Subspace (DIIS) or Pulay mixing (for its inventor Peter Pulay). DIIS extrapolates from the error on past function evaluation to where the error is zero using a subspace approach. It is closely related to other subpsace and Krylov methods, such as GMRES.\nDIIS works best once we are pretty close to our solution. In this case, we start from a significantly better initial guess, and try to achieve a tolerance of \\(\\texttt{1e-10}\\) with just standard gradient descent.\n\n1init={'a':0.18,'b':0.8,'c':-0.1,'d':-0.4}\n2lr=0.1\n\n3no_diis_result=tq.minimize(objective=O1,\n                           lr=lr,\n                           method='sgd',\n                           tol=1e-10,\n                           maxiter=80,\n                           initial_values=init,\n                           silent=True)\n\n\n1\n\nInitialize variables: \\(a = 0.18\\), \\(b = 0.8\\), \\(c = -0.1\\), \\(d = -0.4\\)\n\n2\n\nSet the learning rate to 0.1\n\n3\n\nOptimize the objective \\(O_1\\) using the standard gradient descent method with learning rate 0.1, maximal iterations 80 and a tolerance of 1e-10\n\n\n\n\nLooking at the printout (obtained with silent=False instead of silent=True), we see that convergence is achieved after 46 steps. We note that specifically convergence slows down significantly after 5 steps, as the gradients (and thus step sizes) become smaller the closer we get to the minima. This is the problem DIIS solves.\nLet’s try again with acceleration.\n\n1diis_result=tq.minimize(objective=O1,\n                        lr=lr,\n                        method='sgd',\n2                        diis={'tol':1e-1},\n                        tol=1e-10,\n                        maxiter=80,\n                        initial_values=init,\n                        silent=True)\n\n\n1\n\nOptimize the objective \\(O_1\\) using the standard gradient descent method with learning rate 0.1, maximal iterations 80, a tolerance of 1e-10 and DIIS acceleration\n\n2\n\nSet the DIIS acceleration with a tolerance of 1e-10\n\n\n\n\nWith the printout here (change to silent=False), the convergence is obtained in just 8 steps. We can see here that DIIS kicks in at step 3, that is, when \\(\\texttt{max(gradient)}\\) is below \\(\\texttt{1e-1}\\) (the tol= argument in the DIIS dictionary) and we have accumulated at least 3 vectors. Once DIIS starts, convergence is very rapid, with a four orders of magnitude drop in the error within just three steps.\nWe can see this most clearly in a logarithmic plot of the error.\n\n1import numpy\n                            \nprint('SGD + DIIS optimization results:')                                   \n\n2Emin = diis_result.history.energies[-1]\n\n3plt.semilogy(numpy.asarray(no_diis_result.history.energies[:-1]) - Emin)\n4plt.semilogy(numpy.asarray(diis_result.history.energies[:-1]) - Emin)\n\n5plt.ylabel('Error on energy')\n6plt.legend(['without DIIS', 'with DIIS'])\n\n\n1\n\nImport the numpy library\n\n2\n\nExtract the last energy values from the DISS optimization\n\n3\n\nPlot the error on energy for every step in the optimization without DIIS acceleration\n\n4\n\nPlot the error on energy for every step in the optimization with DIIS acceleration\n\n5\n\nLabel the y-axis of the plot as ‘Error on Energy’\n\n6\n\nAdd a legend to the plot to differentiate between the results with and without DIIS\n\n\n\n\nSGD + DIIS optimization results:\n\n\n&lt;matplotlib.legend.Legend at 0x16eabb100&gt;\n\n\n\n\n\nWe clearly see here that when DIIS kicks in, convergence rapidly accelerates. In comparison, the convergence rate of standard gradient descent actually slows down as we approach a solution. However, it should be noted that DIIS is not entirely stable and only really works close to an optima. Furthermore, the implementation here is somewhat experimental. Please use it at your own risk!\nThe GD optimizer, with the Quantum Natural Gradient:\nThe Quantum Natural Gradient, or QNG, is a novel method of calculating gradients for quantum systems, inspired by the natural gradient sometimes employed in classical machine learning. The usual gradient we employ is with respect to a Euclidean manifold, but this is not the only geometry – nor even the optimal geometry – of quantum space. The QNG is, in essence, a method of taking gradients with respect to (an approximation to) the Fubini-Study metric. For information on how (and why) the QNG is used, see Stokes et.al.\nUsing the QNG in Tequila is as simple as passing in the keyword gradient='qng' to optimizers which support it, such as the GD optimizer. We will use it to optimize \\(O_2\\), our 3 qubit Objective, and then compare the results to optimizing the same circuit with the regular gradient.\n\n1\n2init={k:np.random.uniform(-2,2) for k in ['a','b','c','d']}\n3lr=0.01\n\n4qng_result = tq.minimize(objective=O2,\n5                         gradient='qng',\n                         method='sgd',\n                         maxiter=200,\n                         lr=lr,\n                         initial_values=init,\n                         silent=True)\n\n\n1\n\nkeyword ‘stop_count’ stops optimization if no improvement occurs after 50 epochs\n\n2\n\nInitialize the four variables to random values between -2 and 2\n\n3\n\nSet the learning rate to 0.01\n\n4\n\nOptimize the objective \\(O_2\\) using the standard gradient descent method with learning rate 0.01, maximal iterations 200 and the Quantum Natural Gradient (QNG)\n\n5\n\nSet the gradient to the Quantum Natural Gradient (QNG)\n\n\n\n\n\n1qng_result.history.plot('energies')\n2qng_result.history.plot('angles')\n\nprint('best energy with qng: ',qng_result.energy) \nprint('optimal angles with qng: ',qng_result.angles)\n\n\n1\n\nPlot the energy from the optimization with the Quantum Natural Gradient\n\n2\n\nPlot the angles from the optimization with the Quantum Natural Gradient\n\n\n\n\n\n\n\n\n\n\nbest energy with qng:  -0.6121445376182162\noptimal angles with qng:  a : -0.02215857873260148\nb : 0.0059525380064374455\nc : -0.014680638935248558\nd : -0.0017630474321783914\n\n\n\nTo appreciate the benefits of using the QNG, let’s optimize the same circuit with the same learning rate and method, but without QNG.\n\n1lr=0.01\n\n2sgd_noqng_result = tq.minimize(objective=O2,\n                               gradient=None,\n                               method='sgd',\n                               maxiter=200,\n                               lr=lr,\n                               initial_values=init,\n                               silent=True)\n\nprint('plotting what happens without QNG')\n\n3sgd_noqng_result.history.plot('energies')\n4sgd_noqng_result.history.plot('angles')\n\nprint('best energy without qng: ',sgd_noqng_result.energy)\nprint('optimal angles without qng: ',sgd_noqng_result.angles)\n\n\n1\n\nSet the learning rate to 0.01\n\n2\n\nOptimize the objective \\(O_2\\) using the standard gradient descent method with learning rate 0.01 and maximal iterations 200\n\n3\n\nPlot the energy from the optimization without the Quantum Natural Gradient\n\n4\n\nPlot the angles from the optimization without the Quantum Natural Gradient\n\n\n\n\nplotting what happens without QNG\nbest energy without qng:  -0.2591823908378972\noptimal angles without qng:  a : -0.7272578239465537\nb : 0.7880495712796224\nc : -0.368377646698078\nd : -0.8675949678046627\n\n\n\n\n\n\n\n\n\nAlthough the starting point was random, you will most likely see that the QNG run achieved a greater degree of improvement - it will not perform worse - and that the trajectories followed by the angles were different from those in the SGD-only optimization. Feel free to play around with other methods, learning rates, or circuits in your own code!"
  },
  {
    "objectID": "Tutorials/optimizer/Optimizers.html#the-scipy-optimizer",
    "href": "Tutorials/optimizer/Optimizers.html#the-scipy-optimizer",
    "title": "Optimizer",
    "section": "The SciPy Optimizer",
    "text": "The SciPy Optimizer\nSciPy is one of the most popular optimization packages in Python. It offers a wide variety of optimization strategies. We will not cover them here; for a full exploration of all the SciPy methods, see their documentation. Instead, we will showcase a few of the more powerful options available. Most SciPy keywords like method_options, can be passed directly to minimize in the same way as when using SciPy directly.\n\n1tq.show_available_optimizers(module=\"scipy\")\n\n\n1\n\nGet a list of all available optimization methods for the SciPy optimizer\n\n\n\n\navailable methods for optimizer module scipy\nmethod               | optimizer module\n--------------------------\nNELDER-MEAD          | scipy\nCOBYLA               | scipy\nPOWELL               | scipy\nSLSQP                | scipy\nL-BFGS-B             | scipy\nBFGS                 | scipy\nCG                   | scipy\nTNC                  | scipy\nTRUST-KRYLOV         | scipy\nNEWTON-CG            | scipy\nDOGLEG               | scipy\nTRUST-NCG            | scipy\nTRUST-EXACT          | scipy\nTRUST-CONSTR         | scipy\n\n\nWe will try three different optimizers: COBYLA, which is gradient-free; L-BFGS-B, which employs gradients; and NEWTON-CG, which employs the Hessian.\nAa a reminder, we will optimize:\n\n\n1init={'a':0.25,'b':0.25,'c':0.25,'d':0.25}\n\n2cobyla_result = tq.minimize(objective=O1,\n                            method=\"cobyla\",\n                            initial_values=init,\n                            tol=1.e-3,\n3                            method_options={\"gtol\":1.e-3},\n                            silent=True)\n\n4cobyla_result.history.plot('energies')\n5cobyla_result.history.plot('angles')\n\nprint('best energy with cobyla: ',cobyla_result.energy)\nprint('optimal angles with cobyla: ',cobyla_result.angles)\n\n\n1\n\nInitialize all four variables to fixed values of \\(0.25\\)\n\n2\n\nOptimize the objective \\(O_1\\) using the COBYLA method with a tolerance of \\(1.e-3\\)\n\n3\n\nSet the gradient tolerance to \\(1.e-3\\)\n\n4\n\nPlot the energy from the optimization with the COBYLA method\n\n5\n\nPlot the angles from the optimization with the COBYLA method\n\n\n\n\n\n\n\n\n\n\nbest energy with cobyla:  -1.618031464593162\noptimal angles with cobyla:  a : 2.147373632733447\nb : 1.0010269344671066\nc : -0.0014082403580806177\nd : -0.5000442705117792\n\n\n\n\n1lb_result = tq.minimize(objective=O1,\n                        method=\"l-bfgs-b\",\n                        initial_values=init,\n                        tol=1.e-3,\n2                        method_options={\"gtol\":1.e-3},\n                        silent=True)\n\n3lb_result.history.plot('energies')\n4lb_result.history.plot('angles')\n\nprint('best energy with L-BFGS-B: ',lb_result.energy)\nprint('optimal angles with L-BFGS-B: ',lb_result.angles)\n\n\n1\n\nOptimize the objective \\(O_1\\) using the L-BFGS-B method with a tolerance of \\(1.e-3\\)\n\n2\n\nSet the gradient tolerance to \\(1.e-3\\)\n\n3\n\nPlot the energy from the optimization with the L-BFGS-B method\n\n4\n\nPlot the angles from the optimization with the L-BFGS-B method\n\n\n\n\n\n\n\n\n\n\nbest energy with L-BFGS-B:  -1.6178070274606597\noptimal angles with L-BFGS-B:  a : 0.14354397455393397\nb : 1.0061132343244423\nc : -0.008256863017895754\nd : -0.5036909456037741\n\n\n\n\n1newton_result = tq.minimize(objective=O1,\n                            method=\"newton-cg\",\n                            initial_values=init,\n                            tol=1.e-3,\n2                            method_options={\"gtol\":1.e-3},\n                            silent=True)\n\n3newton_result.history.plot('energies')\n4newton_result.history.plot('angles')\n\nprint('best energy with NEWTON-CG: ',newton_result.energy)\nprint('optimal angles with NEWTON-CG: ',newton_result.angles)\n\n\n1\n\nOptimize the objective \\(O_1\\) using the NEWTON-CG method with a tolerance of \\(1.e-3\\)\n\n2\n\nSet the gradient tolerance to \\(1.e-3\\)\n\n3\n\nPlot the energy from the optimization with the NEWTON-CG method\n\n4\n\nPlot the angles from the optimization with the NEWTON-CG method\n\n\n\n\n\n\n\n\n\n\nbest energy with NEWTON-CG:  -1.618032396266234\noptimal angles with NEWTON-CG:  a : 0.1475346588846646\nb : 1.000870829234684\nc : -0.0011056953828126209\nd : -0.49984554761229166\n\n\n\nAll three of the methods converged to the same minimum, but not necessarily to the same angles; the gradient and Hessian based methods converged to approximately the same angles in similar time.\nScipy Extras: numerical gradients and Hessians\nScipy allows for the use of numerical gradients. To use them, pass down keywords to the gradient argument, such as '2-point'. When using the numerical gradients of SciPy, it is often crucial to determine a feasible step size for the procedure. This can be done with the method_options entry finite_diff_rel_step (for SciPy version 1.5 or higher) or eps (for SciPy version &lt; 1.5).\nHere is one example. Please ensure to check your SciPy version!\n\n1lb_result = tq.minimize(objective=O1,\n                        method=\"l-bfgs-b\",\n                        initial_values=init,\n2                        gradient=\"2-point\",\n                        tol=1.e-3,\n3                        method_options={\"gtol\":1.e-3, \"finite_diff_rel_step\":1.e-4},\n                        silent=True)\n\n4lb_result.history.plot('energies')\n5lb_result.history.plot('angles')\n\nprint('best energy with L-BFGS-B: ',lb_result.energy)\nprint('optimal angles with L-BFGS-B: ',lb_result.angles)\n\n\n1\n\nOptimize the objective \\(O_1\\) using the L-BFGS-B method with a tolerance of \\(1.e-3\\), numerical gradients, and a step size of \\(1.e-4\\)\n\n2\n\nSet the gradient to 2-point to use numerical gradients\n\n3\n\nSet the step size to \\(1.e-4\\)\n\n4\n\nPlot the energy from the optimization with the L-BFGS-B method and numerical gradients\n\n5\n\nPlot the angles from the optimization with the L-BFGS-B method and numerical gradients\n\n\n\n\n\n\n\n\n\n\nbest energy with L-BFGS-B:  -1.6178082631717572\noptimal angles with L-BFGS-B:  a : 0.1435263676387571\nb : 1.0060019972486893\nc : -0.00817836792105437\nd : -0.5036839801675221\n\n\n\nScipy Extras: the QNG in SciPy\nScipy is also configured to use the QNG, just as the GD optimizer is. All one needs to do is set gradient=qng. Let’s see how QNG interacts with the BFGS optimizer. We will use \\(0_2\\), our 3-qubit expectation value, that we used previously.\n\n1init={k:np.random.uniform(-2,2) for k in ['a','b','c','d']}\n2lr=0.01\n\n3bfgs_qng_result = tq.minimize(objective=O2,\n4                              gradient='qng',\n                              method='bfgs',\n                              maxiter=200,\n                              lr=lr,\n                              initial_values=init,\n                              silent=True)\n\nprint('plotting what happens with QNG')\n\n5bfgs_qng_result.history.plot('energies')\n6bfgs_qng_result.history.plot('angles')\n\nprint('best energy with qng: ',bfgs_qng_result.energy)\nprint('optimal angles with qng: ',bfgs_qng_result.angles)\n\n\n1\n\nInitialize the four variables to random values between -2 and 2\n\n2\n\nSet the learning rate to 0.01\n\n3\n\nOptimize the objective \\(O_2\\) using the BFGS method with learning rate 0.01, maximal iterations 200 and the Quantum Natural Gradient (QNG)\n\n4\n\nSet the gradient to the Quantum Natural Gradient (QNG)\n\n5\n\nPlot the energy from the optimization with the BFGS method and the Quantum Natural Gradient\n\n6\n\nPlot the angles from the optimization with the BFGS method and the Quantum Natural Gradient\n\n\n\n\nplotting what happens with QNG\nbest energy with qng:  -0.3535533560030429\noptimal angles with qng:  a : -1.57098151004744\nb : 0.02566573196727211\nc : 1.5704174098342945\nd : 1.5706630278472251\n\n\n\n\n\n\n\n\n\n\n1bfgs_noqng_result = tq.minimize(objective=O2,\n2                                gradient=None,\n                                method='bfgs',\n                                maxiter=200,\n                                lr=lr,\n                                initial_values=init,\n                                silent=True)\n\nprint('plotting what happens without QNG')\n\n3bfgs_noqng_result.history.plot('energies')\n4bfgs_noqng_result.history.plot('angles')\n\nprint('best energy without qng: ',bfgs_noqng_result.energy)\nprint('optimal angles without qng: ',bfgs_noqng_result.angles)\n\n\n1\n\nOptimize the objective \\(O_2\\) using the BFGS method with learning rate 0.01 and maximal iterations 200\n\n2\n\nSet the gradient to None\n\n3\n\nPlot the energy from the optimization with the BFGS method\n\n4\n\nPlot the angles from the optimization with the BFGS method\n\n\n\n\nplotting what happens without QNG\nbest energy without qng:  -0.6123722316299733\noptimal angles without qng:  a : 0.00023318809750098347\nb : -0.0007233623236951418\nc : 5.297705957416901e-05\nd : -0.00029333277886757145"
  },
  {
    "objectID": "Tutorials/KrylovTutorial/KrylovTutorial.html",
    "href": "Tutorials/KrylovTutorial/KrylovTutorial.html",
    "title": "Krylov Method with Tequila",
    "section": "",
    "text": "In this tequila tutorial you can learn how to apply the Krylov method to approximate the ground state of a given Hamiltonian.\n\nTheoretical introduction\nHere we briefly introduce the main idea of the MultiReference Selected Quantum Krylov (MRSQK) method motivating why it is useful to have it implemented in Tequila.\nMRSQK is a low-cost alternative to the quantum phase estimation algorithm that allows to generate the ground state of an hamiltonian as a linear combination of non-orthogonal Krylov basis states \\(\\mathcal{K}_s = \\{\\psi_\\alpha, \\ \\alpha=1, 2, ... , N\\}\\). This basis is usually obtained via real time evolution from a reference set of states.\nSo, given an hamiltonian \\(H\\) and a Krylov basis \\(\\mathcal{K}_s\\), the ground state of \\(H\\) can be written as:\n\\[|\\Psi\\rangle = \\sum_\\alpha c_\\alpha|\\psi_\\alpha\\rangle \\quad .\\]\nThe coefficients \\(c_\\alpha\\) and the ground energy value \\(E\\) can be obtained by solving the following generalized eigenvalue problem: \\[\\mathbf{Hc} = \\mathbf{Sc}E\\] where the elements of the overlap matrix (\\(\\mathbf{S}\\)) and Hamiltonian (\\(\\mathbf{H}\\)) are \\[S_{\\alpha\\beta} = \\langle\\psi_\\alpha|\\psi_\\beta\\rangle \\quad ,\\] \\[H_{\\alpha\\beta} = \\langle\\psi_\\alpha|H|\\psi_\\beta\\rangle \\quad .\\]\nLuckily, with tequila we can easily compute terms like \\(S_{\\alpha\\beta}\\) and \\(H_{\\alpha\\beta}\\) thanks to the tq.braket function. The krylov_method function uses it and allows to easily solve the generalized eigenvalue problem giving as output the energy \\(E\\) and the coefficients \\(c_\\alpha\\).\n\n\nSimple example\n\nimport tequila as tq\nfrom tequila.apps.krylov import krylov_method\nfrom tequila.hamiltonian.qubit_hamiltonian import QubitHamiltonian\nfrom tequila.tools.random_generators import make_random_circuit\nimport itertools as it\nimport numpy as np\n\nHere we present a simple/trivial example in which we apply MRSQK. In order to do this, we create two quantum circuits randomly, \\(|\\psi\\rangle\\) and \\(|\\phi\\rangle\\), and we use these as Krylov basis.\n\n1np.random.seed(111)\n2n_krylov_states = 2\n\nkrylov_circs = [make_random_circuit(2, enable_controls=True) \n3                for i in range(n_krylov_states)]\n\n4krylov_states = [tq.simulate(circ) for circ in krylov_circs]\n\n\n1\n\nSet the random seed for reproducibility\n\n2\n\nNumber of Krylov states to generate\n\n3\n\nCreate random quantum circuits, in this way it is very unlikely they will be orthogonal\n\n4\n\nCreate the wavefunctions from the circuits\n\n\n\n\nThen we build an Hamiltonian from these as follows: \\[H = -|\\psi\\rangle\\langle\\psi|-|\\phi\\rangle\\langle\\psi|-|\\psi\\rangle\\langle\\phi|-|\\phi\\rangle\\langle\\phi|\\] In this way we have an hermitian operator and we are sure that the Krylov space contains the ground state. This is a toy hamiltonian that has nothing to do with the Krylov method itself, it’s only needed to check the obtained states are the correct ones.\n\n1krylov_states_couples = list(it.product(krylov_states, repeat=2))\n\n2\n3H = QubitHamiltonian()\n4for i, j in krylov_states_couples:\n    H -= tq.paulis.KetBra(ket = i, bra = j)\n\n\n1\n\nGenerate a list of all posible couples of Krylov states\n\n2\n\nCreate a Hamiltonian from the obtained wavefunctions\n\n3\n\nInitialize an empty QubitHamiltonian object\n\n4\n\nFor each couple of Krylov states, compute the braket and substract the term from the Hamiltonian\n\n\n\n\nAt this point we just need to call the krylov_method function, providing the Krylov circuits and the hamiltonian. It will build the matrices \\(\\mathbf{S_{\\alpha\\beta}}\\) and \\(\\mathbf{H_{\\alpha\\beta}}\\) and then return the ground energy \\(E\\) and the coefficients \\(c_\\alpha\\):\n\n1kry_energies, kry_coefficients_matrix = krylov_method(krylov_circs, H)\n\n2kry_ground_energy = kry_energies[0]\n3kry_ground_coefficients = kry_coefficients_matrix[:,0]\n\n\n1\n\nApplying the Krylov method\n\n2\n\nExtract the ground state energy\n\n3\n\nExtract the coefficients\n\n\n\n\nIn order to check if the method gives meaningful solutions we can directly diagonalize the hamiltonian \\(H\\):\n\n1eigenvalues, eigenvectors = np.linalg.eigh(H.to_matrix())\n\n\n1\n\nPerform exact diagonalization of the Hamiltonian\n\n\n\n\nAs you can see below the ground energy and the ground states do correspond:\n\nprint('Ground State Energy Krylov: {:.4f}'.format(kry_ground_energy))      \nprint('Ground State Energy: {:.4f}'.format( eigenvalues[0]))               \n\nGround State Energy Krylov: -1.6530\nGround State Energy: -1.6530\n\n\n\n1ground_state = tq.QubitWaveFunction()\n\n2for i in range(n_krylov_states):\n    ground_state += kry_ground_coefficients[i]*krylov_states[i]\n\n3print(ground_state)\n\n\n1\n\nInitialize the ground state\n\n2\n\nConstruct the ground state wavefunction by adding the scaled Krylov states\n\n3\n\nPrint the ground state\n\n\n\n\n+0.7338e^(-0.4020πi)|00&gt; +0.5664i|10&gt; -0.2799|11&gt; +0.2497e^(+0.8983πi)|01&gt; \n\n\n\n1wfn = tq.QubitWaveFunction.from_array(eigenvectors[:,0])\n#print(eigenvectors[0])\nprint(wfn)  \n\n\n1\n\nCreate a QubitWaveFunction object from the array representing the first eigenvector\n\n\n\n\n+0.7338|00&gt; +0.2497e^(-0.6997πi)|01&gt; +0.5664e^(+0.9020πi)|10&gt; +0.2799e^(-0.5980πi)|11&gt; \n\n\nThe obtained ground state seems different at first glance, but the states are actually identical due to equivalence up to a global phase. This depends on the employed simulator that decomposes gates up to a global phase. Using different ones may lead to different global phases, still having the same state.\nWe can easily check that these two are the same state by computing the fidelity between the two:\n\n1fidelity = abs(wfn.inner(ground_state.normalize()))**2\nprint(fidelity)\n\n\n1\n\nCompute the fidelity between the two states\n\n\n\n\n1.0000000000000004"
  },
  {
    "objectID": "Tutorials/Braket/BraketTutorial.html",
    "href": "Tutorials/Braket/BraketTutorial.html",
    "title": "Braket Module",
    "section": "",
    "text": "This is a tutorial to get to know the new Tequila’s function: braket.\nWith this new function you will be able to easily compute:\nSo, let’s import Tequila (and some of its modules) and see how this function works.\n#tequila importations\nimport tequila as tq\nfrom tequila.circuit.gates import H, X, Rx, CX\nfrom tequila.hamiltonian.qubit_hamiltonian import QubitHamiltonian\n\n#extra importations\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline"
  },
  {
    "objectID": "Tutorials/Braket/BraketTutorial.html#self-overlap",
    "href": "Tutorials/Braket/BraketTutorial.html#self-overlap",
    "title": "Braket Module",
    "section": "1. Self overlap",
    "text": "1. Self overlap\nLet’s begin from the simplest (and maybe trivial) possible application of the braket function: the computation of the self overlap of a quantum state. One can prepare an arbitrary circuit, corresponding to the state \\(|\\psi\\rangle\\) and then calculate the self overlap \\[\\langle \\psi|\\psi\\rangle.\\]\nWe can prepare for example a Bell state of two qubits:\nNote you can also run tq.draw(bell, backend='qiskit') for drawing the circuit (optional). It would result in a circuit like the one below\n\n1bell = tq.QCircuit()\n2bell += H(0)\n3bell += CX(0,1)\n\n\n1\n\ncreates an empty circuit object with no gates\n\n2\n\nadd a hadamard gate to qubit 0\n\n3\n\nadd a \\(CNOT\\) with control in qubit 1 and target in 1\n\n\n\n\n\nThen, we can simply call the braket function passing as unique argument our quantum circuit, this can also be specified as ket:\n\n1self_overlap_real, self_overlap_im = tq.braket(ket=bell)\n\n2self_overlap = self_overlap_real + 1.0j*self_overlap_im\n\n\n1\n\nget the real and the imaginary part of the state\n\n2\n\njoin the save and imaginary part of the overlap. self_overlap is not a complex number\n\n\n\n\nAs we have seen the braket function always return two objects, one for the real part and one for the imaginary one. These are tq.Objecitve and need to be simulated in order to obtain a numeric value.\nDue to the laws of quantum mechanics the overlap of a state with itself, namely the self overlap, must always be equal to 1, namely \\[\\langle \\psi|\\psi\\rangle=1.\\]\n\n1self_overlap_value = tq.simulate(self_overlap)\n2assert self_overlap_value == 1\n\n\n1\n\ncompute the self overlap of the state\n\n2\n\nif the self overlap is not equal to \\(1\\), this should throw an exception\n\n\n\n\n/opt/anaconda3/envs/BlogQA/lib/python3.9/site-packages/tequila/objective/objective.py:591: ComplexWarning: Casting complex values to real discards the imaginary part\n  result = onp.asarray(self.transformation(*ev_array), dtype=float)\n\n\nIn this particular case Tequila is aware that the self overlap is always 1, so no circuit needs to be executed. This can be checked by printing the self_overlap Objective:\nNote that this object consists of further attributes rather than only the mere value\n\n1print(self_overlap)\n\n\n1\n\nprinting the overlap shows all of its object attributes\n\n\n\n\nObjective with 0 unique expectation values\ntotal measurements = 0\nvariables          = []\ntypes              = []"
  },
  {
    "objectID": "Tutorials/Braket/BraketTutorial.html#overlap-between-two-different-states",
    "href": "Tutorials/Braket/BraketTutorial.html#overlap-between-two-different-states",
    "title": "Braket Module",
    "section": "2. Overlap between two different states",
    "text": "2. Overlap between two different states\nThe second step will be to compute the overlap between different states \\[\\langle\\psi _2|\\psi _1\\rangle.\\] Even in this case we can exploit the braket function.\nLet’s prepare two orthogonal states. In this case the overlap must be equal to 0:\n\n1U0 = X(0) # state |10&gt;\n2U1 = H(0)+CX(0,1) # Bell state 1/sqrt(2)*(|00&gt;+|11&gt;)\n\n\n1\n\nthis circuit will perpare the two-qubit state |10&gt; since we assume two input-qubits in state |0&gt; and a \\(X\\) (NOT) is applied to the first one\n\n2\n\nalso here we assume an input of |00&gt; and send it through the circuit that generated the 4 bell-states\n\n\n\n\nNow, we can call the braket function passing as arguments our quantum circuits, specifying which will be the ket and which the bra. Otherwise the first will be automatically selected as ket and second as bra:\n\n1br_objective_real, br_objective_im = tq.braket(ket=U0, bra=U1)\n2br_overlap = br_objective_real + 1.0j*br_objective_im\n3br_overlap_value = tq.simulate(br_overlap)\n\n4br_overlap_value\n\n\n1\n\nspecify input states for writing them in the braket notation\n\n2\n\njoining real and imaginary part based on the states produced by the circuits U0 and U1\n\n3\n\nsimulates the tq.Objective object to obtain a numeric values\n\n4\n\nprints the overlap, this one should be equal to \\(0\\)\n\n\n\n\n0.0\n\n\nWe can also define a non-orthogonal state with respect to the previous prepared Bell state. This can be obtained for example by preparing a Bell state followed by a rotation around X-axis.\nOne can then check that changing the angle value will result in changing the overlap between the two states:\n\n1angle = tq.Variable(\"angle\")\n2U2 = U1 + Rx(angle*np.pi, 1)\n\n3br_objective_real, br_objective_im = tq.braket(ket=U1, bra=U2)\n4br_overlap = br_objective_real + 1.0j*br_objective_im\n5S = tq.compile(br_overlap)\n\noverlaps = []\n6for value in np.arange(0,4.1, 0.1):\n# set the value of the angle we want to simulate\n7    variables = {\"angle\" : value}\n    # let's simulate the Objective\n8    br_overlap_value = S(variables=variables)\n9    overlaps.append(round(br_overlap_value,3))\n\n# plotting the trend \n10fig = plt.figure(figsize=(8,5))\n\nplt.plot(np.arange(0,4.1, 0.1), overlaps, marker = 'o')\nplt.xlabel(r'Angle[Units of $\\pi$]')\nplt.ylabel('Overlap')\n\n\n1\n\ndeclare a variable of for a parametrized angle\n\n2\n\nBell state followed by a rotation around X-axis\n\n3\n\nfetch once again the real and imaginary components after writing the input states in braket form\n\n4\n\nprepare real and imaginary part for overlap computation\n\n5\n\ntranslate to backend - compile the state with an arbitrary parametrized angle\n\n6\n\ngenerates values from the intervall \\([0,4.1]\\) with steps of \\(0.1\\) for simulating multiple angles\n\n7\n\nassociate angle with generated value from range above\n\n8\n\nsimulate the tq.Objective object ad get the computed overlap\n\n9\n\nappend the achieved value to a list of overlaps, since we compute multiple overlaps for multiple angles\n\n10\n\ndisplay the resulted numerical values of the overlaps as a graph\n\n\n\n\nText(0, 0.5, 'Overlap')\n\n\n\n\n\nThe resulting graphic depicts the trend of the overlaps for the given states above and angles within the interval \\([0, 4.1]\\) with steps of \\(0.1\\) between interval begin and interval end."
  },
  {
    "objectID": "Tutorials/Braket/BraketTutorial.html#expectation-value-of-an-hamiltonian",
    "href": "Tutorials/Braket/BraketTutorial.html#expectation-value-of-an-hamiltonian",
    "title": "Braket Module",
    "section": "3. Expectation value of an Hamiltonian",
    "text": "3. Expectation value of an Hamiltonian\nAt this stage, we can see how to use the braket function to calculate the expectation value of an Hamiltonian \\(H\\) with respect to a given state \\(|\\psi\\rangle\\): \\[\\langle\\psi|H|\\psi\\rangle.\\]\n\n1psi = H(0)+CX(0,1) # Bell state 1/sqrt(2)*(|00&gt;+|11&gt;)\n\n#Hamiltonian definition\n2operator = QubitHamiltonian.from_string(\"-1.0*X(0)X(1)\")\n\n\n1\n\nas before define the first bell state with the circuit generating the bell-basis and an input of |00&gt;\n\n2\n\ndefine an hamiltonian - this one is a sequence of Pauli-\\(X\\) matrices acting on the second and the first qubits respectivly and then a scale factor of \\(-1\\)\n\n\n\n\nNow, one needs to pass to the braket function as ket argument our state and as operator argument the Hamiltonian.\nAs already seen before, always keep in mind that the braket function returns two Objecitves that need to be compiled afterwards.\nWith previous defined state and Hamiltonian we need to obtain an expectation value equal to \\(-1\\).\nNote: now we utilize the braket function for computing an expectation value and not an overlap between two states as we did before, but since we provide the function this time with an hamiltonian = hermitian operator and a state instead of two states, the expectation value will be computed and not the overlap - no special changes needed.\n\n1br_exp_value_real, br_exp_value_im = tq.braket(ket=psi, operator=operator)\n2br_exp_value_tmp = br_exp_value_real + 1.0j*br_exp_value_im\n3br_exp_value = tq.simulate(br_exp_value_tmp)\n\n4br_exp_value\n\n\n1\n\napplying the braket function on the bell-state and the hamiltonian, defining the hamiltonian as an operator - important\n\n2\n\njoining real and imaginary part for computation\n\n3\n\nsimulate the expectation value\n\n4\n\nshow the expectatino value, should be equal to \\(1\\)\n\n\n\n\n-0.9999999999999998\n\n\nTequila is aware that an ExpectationValue is always real, so no circuit needs to be executed for the imaginary part. One can see this by printing br_exp_value_im:\n\nprint(br_exp_value_im)\n\nObjective with 0 unique expectation values\ntotal measurements = 0\nvariables          = []\ntypes              = []"
  },
  {
    "objectID": "Tutorials/Braket/BraketTutorial.html#transition-element-of-an-hamiltonian",
    "href": "Tutorials/Braket/BraketTutorial.html#transition-element-of-an-hamiltonian",
    "title": "Braket Module",
    "section": "4. Transition element of an Hamiltonian",
    "text": "4. Transition element of an Hamiltonian\nAs last application, the braket function can be used to calculate transition elements of an operator \\(H\\) with respect to two different states \\(|\\alpha\\rangle, \\ |\\beta\\rangle\\): \\[\\langle\\beta|H|\\alpha\\rangle.\\]\nIn order to see how things can change, we can proceed similarly to section 2 and vary the angle of a rotation:\n\n#states definition\n1alpha = H(0)+CX(0,1) # Bell state 1/sqrt(2)*(|00&gt;+|11&gt;)\n2angle = tq.Variable(\"angle\")\n3beta = alpha + Rx(angle*np.pi, 1) # Bell state followed by a rotation around X-axis\n\n#Hamiltonian definition\n4operator = QubitHamiltonian(\"-1.0*X(0)X(1)\")\n\n5br_trans_real, br_trans_im = tq.braket(ket=alpha, bra=beta, operator=operator)\n6br_trans_el = br_trans_real + 1.0j*br_trans_im\n7H_ab = tq.compile(br_trans_el)\n\ntransitions = []    \n8for value in np.arange(0,4.1, 0.1):\n9    variables = {\"angle\" : value}\n10    br_trans_value = H_ab(variables=variables)\n11    transitions.append(round(br_trans_value,3))\n\n\n1\n\ncreate the bell state the same way as before\n\n2\n\ndefine a general angle for later computations as in the last example\n\n3\n\nadd a rotation after the bell state\n\n4\n\ndefine the hamiltonian the same way as before\n\n5\n\napply braket function, specify the ket and the bra as the bell state and the bell state followed by the \\(X\\) rotation respectively and get the real and imaginary part\n\n6\n\nuse the real and imaginary part for creating a complex number\n\n7\n\ncompile this complex number by translating it to the backend\n\n8\n\nuse the same interval as a range for angle generation as before and the same step size\n\n9\n\nset the of the angle we want to simulate\n\n10\n\nsimulate the tq.Objective to get the computed value\n\n11\n\nstore these values rounded by 3 digits after the comma into a list of transitions for later plot\n\n\n\n\n\n#plotting the trend\nfig = plt.figure(figsize=(8,5)) \nplt.plot(np.arange(0,4.1, 0.1), transitions, marker = 'o')\nplt.xlabel(r'Angle[$\\pi$]')\nplt.ylabel('Transitions')\n\nText(0, 0.5, 'Transitions')\n\n\n\n\n\nEven in this particular case the imaginary part is 0 (as for an ExpectationValue), but in this case this happens because the wave functions are real. The difference can be noticed again by printing br_trans_im where in this case there is one circuit to be executed:\n\nprint(br_trans_im)\n\nObjective with 1 unique expectation values\ntotal measurements = 1\nvariables          = [angle]\ntypes              = not compiled\n\n\nIn the end, we can address one small example in which the imaginary part is not 0. We have: \\[|\\psi_0\\rangle=R_x(a)\\rvert  0 \\rangle = \\cos(a/2) \\rvert 0 \\rangle - i\\sin(a/2)\\rvert 1\\rangle\\] \\[|\\psi_1\\rangle=R_y(b)\\rvert  0 \\rangle = \\cos(b/2) \\rvert 0 \\rangle + \\sin(b/2)\\rvert 1\\rangle\\]\nand so the imaginary part of the transition amplitude comes down to \\[\\text{Im}(\\langle\\psi_1|Z|\\psi_0\\rangle ) = -\\sin(a/2)\\sin(b/2).\\]\n\n#states definition\n1a = tq.Variable(\"a\")\nb = 1 \n\n2U0 = tq.gates.Rx(a*np.pi,0)\n3U1 = tq.gates.Ry(b*np.pi,0)\n\n#Hamiltonian definition\n4operator = QubitHamiltonian(\"1.0*Z(0)\")\n\n5br_trans_real, br_trans_im = tq.braket(ket=U0, bra=U1, operator=operator)\n\n6H_i = tq.compile(br_trans_im) #\n\ntransitions = []\n7for a in np.arange(0,4.1, 0.1):\n8    variables = {\"a\" : a}\n9    br_trans_value = H_i(variables=variables)\n10    transitions.append(round(br_trans_value,3))\n\n\n1\n\ndeclare a variable of a general angle \\(a\\)\n\n2\n\ndeclare a rotation around \\(X\\) with the angle \\(a\\pi\\)\n\n3\n\nrotation around \\(Y\\) axis with a fixed angle of \\(\\pi\\)\n\n4\n\nhamiltonian with a paulii \\(Z\\) gate on the first qubit\n\n5\n\nspecify ket, bra and operator and fetch the imaginary and real part\n\n6\n\ntransalte to backend and get the operator\n\n7\n\nsame range and step size as before\n\n8\n\nset the angle values we want to simulate\n\n9\n\nsimulate the tq.Objective to get the computed value\n\n10\n\nstore the computed values in a list for plotting\n\n\n\n\n\n#plotting the trend\nfig = plt.figure(figsize=(8,5))\nplt.plot(np.arange(0,4.1, 0.1), transitions, marker = 'o')\nplt.xlabel(r'Angle a [$\\pi$]')\nplt.ylabel('Transitions')\n\nText(0, 0.5, 'Transitions')"
  },
  {
    "objectID": "Tutorials/Braket/BraketTutorial.html#self-overlap-1",
    "href": "Tutorials/Braket/BraketTutorial.html#self-overlap-1",
    "title": "Braket Module",
    "section": "Self Overlap",
    "text": "Self Overlap\n\nIf only ket is passed, braket directly returns the real and imaginary part of the self-overlap as Objective().\nThis is required since one may combine the different braket modalities and this will require the compilation of an Objective()."
  },
  {
    "objectID": "Tutorials/Braket/BraketTutorial.html#expectation-value",
    "href": "Tutorials/Braket/BraketTutorial.html#expectation-value",
    "title": "Braket Module",
    "section": "Expectation Value",
    "text": "Expectation Value\n\nIf ket and operator are passed, braket returns the ExpectationValue of the operator for the given state."
  },
  {
    "objectID": "Tutorials/Braket/BraketTutorial.html#general-overlap",
    "href": "Tutorials/Braket/BraketTutorial.html#general-overlap",
    "title": "Braket Module",
    "section": "General Overlap",
    "text": "General Overlap\n\nIf ket and bra are passed, braket returns the overlap between the two states. This is done by calling a new Tequila’s function called make_overlap. Further below this section we describe also how this function works."
  },
  {
    "objectID": "Tutorials/Braket/BraketTutorial.html#transition-element",
    "href": "Tutorials/Braket/BraketTutorial.html#transition-element",
    "title": "Braket Module",
    "section": "Transition Element",
    "text": "Transition Element\n\nIf ket, bra and operator are passed, braket returns the transition element of the operator by calling another new Tequila’s function called make_transition. Even in this case we provide a description of the internal structure."
  },
  {
    "objectID": "Tutorials/OpenQASMConversions/OpenQASMConversions.html",
    "href": "Tutorials/OpenQASMConversions/OpenQASMConversions.html",
    "title": "OpenQASMConversions",
    "section": "",
    "text": "In this tutorial, we’ll introduce OpenQASM, a key language for quantum programming and its use within IBM’s Qiskit framework."
  },
  {
    "objectID": "Tutorials/OpenQASMConversions/OpenQASMConversions.html#example",
    "href": "Tutorials/OpenQASMConversions/OpenQASMConversions.html#example",
    "title": "OpenQASMConversions",
    "section": "Example",
    "text": "Example\nThe following is an example of OpenQASM source code from the official library. The program adds two four-bit numbers.\n// quantum ripple-carry adder from Cuccaro et al, quant-ph/0410184\nOPENQASM 2.0;\ninclude \"qelib1.inc\";\ngate majority a,b,c \n{ \n  cx c,b; \n  cx c,a; \n  ccx a,b,c; \n}\ngate unmaj a,b,c \n{ \n  ccx a,b,c; \n  cx c,a; \n  cx a,b; \n}\nqreg cin[1];\nqreg a[4];\nqreg b[4];\nqreg cout[1];\ncreg ans[5];\n// set input states\nx a[0]; // a = 0001\nx b;    // b = 1111\n// add a to b, storing result in b\nmajority cin[0],b[0],a[0];\nmajority a[0],b[1],a[1];\nmajority a[1],b[2],a[2];\nmajority a[2],b[3],a[3];\ncx a[3],cout[0];\nunmaj a[2],b[3],a[3];\nunmaj a[1],b[2],a[2];\nunmaj a[0],b[1],a[1];\nunmaj cin[0],b[0],a[0];\nmeasure b[0] -&gt; ans[0];\nmeasure b[1] -&gt; ans[1];\nmeasure b[2] -&gt; ans[2];\nmeasure b[3] -&gt; ans[3];\nmeasure cout[0] -&gt; ans[4];"
  },
  {
    "objectID": "Tutorials/tq-circuits/index.html",
    "href": "Tutorials/tq-circuits/index.html",
    "title": "Tequila Basics: Circuits",
    "section": "",
    "text": "In this tutorial, you will get a brief introduction to tequila circuits."
  },
  {
    "objectID": "Tutorials/tq-circuits/index.html#add-controls",
    "href": "Tutorials/tq-circuits/index.html#add-controls",
    "title": "Tequila Basics: Circuits",
    "section": "Add Controls",
    "text": "Add Controls\nYou can take one circuit and add control-options to it\n\ncU = U.add_controls([3,4])\n\n\n\n\noriginal\n\n\n\n\n\nadded controls"
  },
  {
    "objectID": "Tutorials/tq-circuits/index.html#re-parametrize",
    "href": "Tutorials/tq-circuits/index.html#re-parametrize",
    "title": "Tequila Basics: Circuits",
    "section": "Re-Parametrize",
    "text": "Re-Parametrize\n\nUX = U.map_variables({\"a\":\"b\"})\nUY = UX.map_variables({\"b\":1.0})\nprint(\"angles in U :\",U.extract_variables())\nprint(\"angles in UX:\",UX.extract_variables())\nprint(\"angles in UY:\",UY.extract_variables())\n\nangles in U : [a]\nangles in UX: [b]\nangles in UY: []"
  },
  {
    "objectID": "Tutorials/tq-circuits/index.html#map-qubits",
    "href": "Tutorials/tq-circuits/index.html#map-qubits",
    "title": "Tequila Basics: Circuits",
    "section": "Map-Qubits",
    "text": "Map-Qubits\n\nUZ = U.map_qubits({0:1,1:2}) \nprint(UZ)\n\ncircuit: \nRy(target=(1,), parameter=f([a]))\nX(target=(2,), control=(1,))"
  },
  {
    "objectID": "Tutorials/tq-circuits/index.html#decompose-gates",
    "href": "Tutorials/tq-circuits/index.html#decompose-gates",
    "title": "Tequila Basics: Circuits",
    "section": "Decompose Gates",
    "text": "Decompose Gates\nTequila can decompose gates automatically (e.g. for differentiation or translation to backend), you can however also do this manually with tq.compile_circuit. The default compiles down to the lowest lever (only standard single qubit gates and controlled-nots). You can specifiy the compile options with keywords like controlled_rotation=False. See here for all keywords.\n\nU1 = tq.gates.H(0)\nU1+= tq.gates.Ry(\"a\",0,control=1) \nU1+= tq.gates.ExpPauli(angle=\"b\", paulistring=\"X(0)X(2)\")\n# let's compile to different levels\nU2 = tq.compile_circuit(U1)\nU3 = tq.compile_circuit(U1, controlled_rotation=False)\nU4 = tq.compile_circuit(U1, exponential_pauli=False)\n\nthe four circuits are\n\n\n\nU1\n\n\n\n\n\nU2 (full compile)\n\n\n\n\n\nU3 (conrolled_rotation=False)\n\n\n\n\n\nU4 (exponential_pauli=False)"
  },
  {
    "objectID": "Tutorials/tq-circuits/index.html#get-generators",
    "href": "Tutorials/tq-circuits/index.html#get-generators",
    "title": "Tequila Basics: Circuits",
    "section": "Get Generators",
    "text": "Get Generators\nThe generators of individual gates can be constructed with the make_generator function. Here we can include the control qubits in the definition of the generator. Take for example\n\nU = tq.gates.Ry(angle=\"a\", target=0) + tq.gates.X(control=0, target=1) \nfor gate in U.gates:\n    generator = gate.make_generator()\n    print(\"gate generated by:\", generator)\n    if gate.is_controlled():\n        cgenerator = gate.make_generator(include_controls=True)\n        print(\"including control qubits:\", cgenerator)\n\ngate generated by: +1.0000Y(0)\ngate generated by: +1.0000X(1)-1.0000\nincluding control qubits: +0.5000X(1)-0.5000-0.5000Z(0)X(1)+0.5000Z(0)"
  },
  {
    "objectID": "FAQ.html",
    "href": "FAQ.html",
    "title": "FAQ",
    "section": "",
    "text": "FAQ\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nAug 2, 2024\n\n\nElisabeth Welizky\n\n\n\n\n\n\nNo matching items"
  }
]